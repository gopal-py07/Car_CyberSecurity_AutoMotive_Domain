{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21477a8d-ccb0-4f3d-989b-e39c2f33b483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T06:42:23.313979Z",
     "iopub.status.busy": "2024-09-19T06:42:23.313979Z",
     "iopub.status.idle": "2024-09-19T06:42:34.974650Z",
     "shell.execute_reply": "2024-09-19T06:42:34.972325Z",
     "shell.execute_reply.started": "2024-09-19T06:42:23.313979Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 12:12:23,352 - INFO - Starting data preprocessing for anomaly detection.\n",
      "2024-09-19 12:12:23,352 - INFO - Starting data preprocessing for anomaly detection.\n",
      "2024-09-19 12:12:23,394 - INFO - Datasets loaded successfully with the first 1000 rows from each file.\n",
      "2024-09-19 12:12:23,394 - INFO - Datasets loaded successfully with the first 1000 rows from each file.\n",
      "2024-09-19 12:12:23,406 - INFO - Datasets combined successfully.\n",
      "2024-09-19 12:12:23,406 - INFO - Datasets combined successfully.\n",
      "2024-09-19 12:12:23,415 - WARNING - Dataset contains NaN values. Filling with mean.\n",
      "2024-09-19 12:12:23,415 - WARNING - Dataset contains NaN values. Filling with mean.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m combined_data\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     58\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset contains NaN values. Filling with mean.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m     combined_data\u001b[38;5;241m.\u001b[39mfillna(combined_data\u001b[38;5;241m.\u001b[39mmean(), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Check for infinite values and replace them\u001b[39;00m\n\u001b[0;32m     62\u001b[0m combined_data\u001b[38;5;241m.\u001b[39mreplace([np\u001b[38;5;241m.\u001b[39minf, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf], np\u001b[38;5;241m.\u001b[39mnan, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11680\u001b[0m, in \u001b[0;36mDataFrame.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11672\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m  11673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  11674\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11678\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11679\u001b[0m ):\n\u001b[1;32m> 11680\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mmean(axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m  11681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[0;32m  11682\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12417\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  12411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  12412\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  12415\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  12416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 12417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  12418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  12419\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12374\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  12370\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[0;32m  12372\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m> 12374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[0;32m  12375\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[0;32m  12376\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11549\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  11545\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m  11547\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  11548\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 11549\u001b[0m res \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreduce(blk_func)\n\u001b[0;32m  11550\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m  11551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1500\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1498\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m-> 1500\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mreduce(func)\n\u001b[0;32m   1501\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[0;32m   1503\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:404\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    407\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11468\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m  11466\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[0;32m  11467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m> 11468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[1;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[0;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    716\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m    718\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[1;32m--> 719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[0;32m    720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to set up logging\n",
    "def setup_logging(debug=False):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    file_handler = logging.FileHandler('../../logs/data_preprocessing_anomaly_detection1.log')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    if debug:\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "    else:\n",
    "        logger.info(\"Console logging is disabled; only logging to file.\")\n",
    "\n",
    "setup_logging(debug=True)  # Set to True to enable console logging, False for file only\n",
    "\n",
    "logging.info('Starting data preprocessing for anomaly detection.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=1000)\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=1000)\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=1000)\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=1000)\n",
    "    logging.info('Datasets loaded successfully with the first 1000 rows from each file.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Check for NaN or infinite values and handle them\n",
    "if combined_data.isna().sum().sum() > 0:\n",
    "    logging.warning('Dataset contains NaN values. Filling with mean.')\n",
    "    combined_data.fillna(combined_data.mean(), inplace=True)\n",
    "\n",
    "# Check for infinite values and replace them\n",
    "combined_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "combined_data.fillna(combined_data.mean(), inplace=True)\n",
    "\n",
    "logging.info(\"Checked for NaN and infinite values in the dataset.\")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "logging.info('Numeric data normalized using StandardScaler.')\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "logging.info('Data concatenated and converted to dense format for PyTorch.')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Step 1: Train-Test Split\n",
    "X_train, X_test = train_test_split(X_tensor, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Data split into training and test sets with a test size of 20%.\")\n",
    "\n",
    "# Create DataLoaders for both training and test sets\n",
    "train_dataset = TensorDataset(X_train)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "logging.info('DataLoader created for both training and test sets.')\n",
    "\n",
    "# Step 2: Define the Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid to ensure outputs are between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "input_dim = X_tensor.shape[1]  # Number of input features\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "logging.info(f\"Autoencoder model initialized with input dimension: {input_dim}.\")\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "logging.info('Loss function and optimizer set up.')\n",
    "\n",
    "# Train the Autoencoder\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Compare output with input\n",
    "        \n",
    "        # Check if loss is NaN\n",
    "        if torch.isnan(loss):\n",
    "            logging.error(f\"NaN loss encountered at epoch {epoch + 1}. Stopping training.\")\n",
    "            break\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    logging.info(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "logging.info('Training complete.')\n",
    "\n",
    "# Step 4: Anomaly Detection\n",
    "def detect_anomalies(data_loader, model):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs = data[0]\n",
    "            outputs = model(inputs)\n",
    "            reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "            reconstruction_errors += reconstruction_error.cpu().numpy().tolist()\n",
    "    \n",
    "    return np.array(reconstruction_errors)\n",
    "\n",
    "# Step 5: Dynamic Threshold Setting using Percentile\n",
    "def determine_threshold(reconstruction_errors, percentile=95):\n",
    "    # Ensure we don't have NaN reconstruction errors\n",
    "    reconstruction_errors = reconstruction_errors[~np.isnan(reconstruction_errors)]\n",
    "    threshold = np.percentile(reconstruction_errors, percentile)\n",
    "    logging.info(f\"Dynamic threshold set at the {percentile}th percentile: {threshold}\")\n",
    "    return threshold\n",
    "\n",
    "# Plot reconstruction errors and mark the threshold\n",
    "def plot_reconstruction_errors(reconstruction_errors, threshold):\n",
    "    # Ensure no NaN values\n",
    "    clean_reconstruction_errors = reconstruction_errors[~np.isnan(reconstruction_errors)]\n",
    "    \n",
    "    plt.hist(clean_reconstruction_errors, bins=50, alpha=0.7, label='Reconstruction Error')\n",
    "    plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.4f})')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Number of Data Points')\n",
    "    plt.title('Reconstruction Errors and Anomaly Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Call the anomaly detection and threshold determination\n",
    "reconstruction_errors = detect_anomalies(test_loader, autoencoder)\n",
    "threshold = determine_threshold(reconstruction_errors, percentile=95)\n",
    "\n",
    "# Plot the reconstruction errors with the dynamic threshold\n",
    "plot_reconstruction_errors(reconstruction_errors, threshold)\n",
    "\n",
    "# Detect anomalies using the dynamically determined threshold\n",
    "anomalies = reconstruction_errors > threshold\n",
    "num_anomalies = np.sum(anomalies)\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "logging.info(f\"Number of anomalies detected: {num_anomalies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed72e35-0e5f-49e0-876f-18302a59fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
