{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a298bde-477d-47e9-bf50-2e66da22b018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T10:30:43.978215Z",
     "iopub.status.busy": "2024-09-16T10:30:43.978215Z",
     "iopub.status.idle": "2024-09-16T10:30:52.378767Z",
     "shell.execute_reply": "2024-09-16T10:30:52.376980Z",
     "shell.execute_reply.started": "2024-09-16T10:30:43.978215Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '018f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scaler\u001b[38;5;241m.\u001b[39mfit_transform(data)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Assuming the dataset doesn't have labels\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m X \u001b[38;5;241m=\u001b[39m preprocess_data(data\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m     20\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m train_test_split(X, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(data):\n\u001b[0;32m     13\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scaler\u001b[38;5;241m.\u001b[39mfit_transform(data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    913\u001b[0m     X,\n\u001b[0;32m    914\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    915\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    916\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    917\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    918\u001b[0m )\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '018f'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('../data/DoS_dataset.csv')  # Replace with your actual dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "# Assuming the dataset doesn't have labels\n",
    "X = preprocess_data(data.values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the autoencoder model\n",
    "def build_autoencoder(input_dim):\n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    encoder = layers.Dense(64, activation=\"relu\")(input_layer)\n",
    "    encoder = layers.Dense(32, activation=\"relu\")(encoder)\n",
    "    encoder = layers.Dense(16, activation=\"relu\")(encoder)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = layers.Dense(32, activation=\"relu\")(encoder)\n",
    "    decoder = layers.Dense(64, activation=\"relu\")(decoder)\n",
    "    decoder = layers.Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
    "    \n",
    "    # Full model\n",
    "    autoencoder = models.Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # Number of features in the dataset\n",
    "autoencoder = build_autoencoder(input_dim)\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Anomaly Detection\n",
    "def detect_anomalies(data, model, threshold=0.1):\n",
    "    reconstructions = model.predict(data)\n",
    "    reconstruction_loss = np.mean(np.abs(data - reconstructions), axis=1)\n",
    "    anomalies = reconstruction_loss > threshold\n",
    "    return anomalies\n",
    "\n",
    "# Detect anomalies in the test data\n",
    "anomalies = detect_anomalies(X_test, autoencoder)\n",
    "\n",
    "print(\"Detected anomalies:\", anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b7254d-73f9-4e9c-9a8d-e5d0ee89110f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T10:34:16.029614Z",
     "iopub.status.busy": "2024-09-16T10:34:16.029614Z",
     "iopub.status.idle": "2024-09-16T10:34:22.816710Z",
     "shell.execute_reply": "2024-09-16T10:34:22.815701Z",
     "shell.execute_reply.started": "2024-09-16T10:34:16.029614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1478198376.389427    float64\n",
      "0316                  object\n",
      "8                      int64\n",
      "05                    object\n",
      "21                    object\n",
      "68                    object\n",
      "09                    object\n",
      "21.1                  object\n",
      "21.2                  object\n",
      "00                    object\n",
      "6f                    object\n",
      "R                     object\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'message_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'message_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Assuming that 'message_id' is the non-numeric column and needs encoding\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Replace 'message_id' with the actual non-numeric column in your dataset\u001b[39;00m\n\u001b[0;32m     16\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m---> 17\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage_id_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Drop the original non-numeric column (optional)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'message_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the dataset (replace with your dataset path)\n",
    "data = pd.read_csv('../data/DoS_dataset.csv')\n",
    "\n",
    "# Check data types to identify non-numeric columns\n",
    "print(data.dtypes)\n",
    "\n",
    "# Assuming that 'message_id' is the non-numeric column and needs encoding\n",
    "# Replace 'message_id' with the actual non-numeric column in your dataset\n",
    "label_encoder = LabelEncoder()\n",
    "data['message_id_encoded'] = label_encoder.fit_transform(data['message_id'])\n",
    "\n",
    "# Drop the original non-numeric column (optional)\n",
    "data = data.drop(columns=['message_id'])\n",
    "\n",
    "# Preprocess the numeric data (standard scaling)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data.values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the autoencoder model\n",
    "def build_autoencoder(input_dim):\n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    encoder = layers.Dense(64, activation=\"relu\")(input_layer)\n",
    "    encoder = layers.Dense(32, activation=\"relu\")(encoder)\n",
    "    encoder = layers.Dense(16, activation=\"relu\")(encoder)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = layers.Dense(32, activation=\"relu\")(encoder)\n",
    "    decoder = layers.Dense(64, activation=\"relu\")(decoder)\n",
    "    decoder = layers.Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
    "    \n",
    "    # Full model\n",
    "    autoencoder = models.Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # Number of features in the dataset\n",
    "autoencoder = build_autoencoder(input_dim)\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Anomaly Detection based on reconstruction loss\n",
    "def detect_anomalies(data, model, threshold=0.1):\n",
    "    reconstructions = model.predict(data)\n",
    "    reconstruction_loss = np.mean(np.abs(data - reconstructions), axis=1)\n",
    "    anomalies = reconstruction_loss > threshold\n",
    "    return anomalies, reconstruction_loss\n",
    "\n",
    "# Detect anomalies in the test data\n",
    "anomalies, reconstruction_loss = detect_anomalies(X_test, autoencoder)\n",
    "\n",
    "# Display anomalies and corresponding reconstruction loss\n",
    "print(\"Anomalies detected:\", anomalies)\n",
    "print(\"Reconstruction loss:\", reconstruction_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24555d31-72ed-49f6-b3b9-89aa49b99d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T10:36:59.413225Z",
     "iopub.status.busy": "2024-09-16T10:36:59.413225Z",
     "iopub.status.idle": "2024-09-16T10:37:05.193930Z",
     "shell.execute_reply": "2024-09-16T10:37:05.193930Z",
     "shell.execute_reply.started": "2024-09-16T10:36:59.413225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['1478198376.389427', '0316', '8', '05', '21', '68', '09', '21.1',\n",
      "       '21.2', '00', '6f', 'R'],\n",
      "      dtype='object')\n",
      "1478198376.389427    float64\n",
      "0316                  object\n",
      "8                      int64\n",
      "05                    object\n",
      "21                    object\n",
      "68                    object\n",
      "09                    object\n",
      "21.1                  object\n",
      "21.2                  object\n",
      "00                    object\n",
      "6f                    object\n",
      "R                     object\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '018f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Step 3: Preprocess the data (scaling)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 31\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(data\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Split the data into training and testing sets\u001b[39;00m\n\u001b[0;32m     34\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m train_test_split(X, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    913\u001b[0m     X,\n\u001b[0;32m    914\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    915\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    916\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    917\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    918\u001b[0m )\n\u001b[0;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '018f'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the dataset (replace with your dataset path)\n",
    "data = pd.read_csv('../data/DoS_dataset.csv')\n",
    "\n",
    "# Step 1: Inspect the column names to find non-numeric columns\n",
    "print(\"Columns in the dataset:\", data.columns)\n",
    "\n",
    "# Step 2: Identify and handle non-numeric columns\n",
    "# For example, let's assume the non-numeric column is 'ID' (replace 'ID' with the actual column name you want to encode)\n",
    "\n",
    "# Check the data types of columns\n",
    "print(data.dtypes)\n",
    "\n",
    "# Use Label Encoding for non-numeric columns if any\n",
    "# Replace 'ID' with the actual non-numeric column name\n",
    "if 'ID' in data.columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['ID_encoded'] = label_encoder.fit_transform(data['ID'])\n",
    "    \n",
    "    # Drop the original non-numeric column\n",
    "    data = data.drop(columns=['ID'])\n",
    "\n",
    "# Step 3: Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data.values)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Build the autoencoder model\n",
    "def build_autoencoder(input_dim):\n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    encoder = layers.Dense(64, activation=\"relu\")(input_layer)\n",
    "    encoder = layers.Dense(32, activation=\"relu\")(encoder)\n",
    "    encoder = layers.Dense(16, activation=\"relu\")(encoder)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = layers.Dense(32, activation=\"relu\")(encoder)\n",
    "    decoder = layers.Dense(64, activation=\"relu\")(decoder)\n",
    "    decoder = layers.Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
    "    \n",
    "    # Full model\n",
    "    autoencoder = models.Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train.shape[1]  # Number of features in the dataset\n",
    "autoencoder = build_autoencoder(input_dim)\n",
    "\n",
    "# Step 5: Train the model\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Step 6: Anomaly Detection based on reconstruction loss\n",
    "def detect_anomalies(data, model, threshold=0.1):\n",
    "    reconstructions = model.predict(data)\n",
    "    reconstruction_loss = np.mean(np.abs(data - reconstructions), axis=1)\n",
    "    anomalies = reconstruction_loss > threshold\n",
    "    return anomalies, reconstruction_loss\n",
    "\n",
    "# Detect anomalies in the test data\n",
    "anomalies, reconstruction_loss = detect_anomalies(X_test, autoencoder)\n",
    "\n",
    "# Step 7: Display the detected anomalies and corresponding reconstruction loss\n",
    "print(\"Anomalies detected:\", anomalies)\n",
    "print(\"Reconstruction loss:\", reconstruction_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210fc16-31e2-45a3-9963-c844f16f957d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T15:04:38.559225Z",
     "iopub.status.busy": "2024-09-16T15:04:38.558226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['1478198376.389427', '0316', '8', '05', '21', '68', '09', '21.1',\n",
      "       '21.2', '00', '6f', 'R'],\n",
      "      dtype='object')\n",
      "1478198376.389427    float64\n",
      "0316                  object\n",
      "8                      int64\n",
      "05                    object\n",
      "21                    object\n",
      "68                    object\n",
      "09                    object\n",
      "21.1                  object\n",
      "21.2                  object\n",
      "00                    object\n",
      "6f                    object\n",
      "R                     object\n",
      "dtype: object\n",
      "Encoding column: 0316\n",
      "Encoding column: 05\n",
      "Encoding column: 21\n",
      "Encoding column: 68\n",
      "Encoding column: 09\n",
      "Encoding column: 21.1\n",
      "Encoding column: 21.2\n",
      "Encoding column: 00\n",
      "Encoding column: 6f\n",
      "Encoding column: R\n",
      "Epoch 1/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 3ms/step - loss: 0.5786 - val_loss: 0.5714\n",
      "Epoch 2/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 3ms/step - loss: 0.5702 - val_loss: 0.5703\n",
      "Epoch 3/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 3ms/step - loss: 0.5696 - val_loss: 0.5702\n",
      "Epoch 4/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 3ms/step - loss: 0.5704 - val_loss: 0.5702\n",
      "Epoch 5/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 3ms/step - loss: 0.5675 - val_loss: 0.5620\n",
      "Epoch 6/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 3ms/step - loss: 0.5604 - val_loss: 0.5620\n",
      "Epoch 7/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 3ms/step - loss: 0.5615 - val_loss: 0.5620\n",
      "Epoch 8/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 3ms/step - loss: 0.5617 - val_loss: 0.5620\n",
      "Epoch 9/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 3ms/step - loss: 0.5600 - val_loss: 0.5620\n",
      "Epoch 10/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 3ms/step - loss: 0.5621 - val_loss: 0.5620\n",
      "Epoch 11/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 2ms/step - loss: 0.5621 - val_loss: 0.5620\n",
      "Epoch 12/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 2ms/step - loss: 0.5608 - val_loss: 0.5620\n",
      "Epoch 13/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2ms/step - loss: 0.5609 - val_loss: 0.5620\n",
      "Epoch 14/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 2ms/step - loss: 0.5606 - val_loss: 0.5620\n",
      "Epoch 15/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2ms/step - loss: 0.5624 - val_loss: 0.5658\n",
      "Epoch 16/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 2ms/step - loss: 0.5650 - val_loss: 0.5620\n",
      "Epoch 17/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2ms/step - loss: 0.5606 - val_loss: 0.5620\n",
      "Epoch 18/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2ms/step - loss: 0.5609 - val_loss: 0.5620\n",
      "Epoch 19/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 2ms/step - loss: 0.5609 - val_loss: 0.5620\n",
      "Epoch 20/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2ms/step - loss: 0.5617 - val_loss: 0.5620\n",
      "Epoch 21/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2ms/step - loss: 0.5603 - val_loss: 0.5621\n",
      "Epoch 22/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2ms/step - loss: 0.5624 - val_loss: 0.5620\n",
      "Epoch 23/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 2ms/step - loss: 0.5611 - val_loss: 0.5620\n",
      "Epoch 24/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2ms/step - loss: 0.5615 - val_loss: 0.5620\n",
      "Epoch 25/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 2ms/step - loss: 0.5579 - val_loss: 0.5592\n",
      "Epoch 26/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 2ms/step - loss: 0.5574 - val_loss: 0.5522\n",
      "Epoch 27/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2ms/step - loss: 0.5516 - val_loss: 0.5522\n",
      "Epoch 28/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 3ms/step - loss: 0.5555 - val_loss: 0.5647\n",
      "Epoch 29/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 3ms/step - loss: 0.5581 - val_loss: 0.5558\n",
      "Epoch 30/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 3ms/step - loss: 0.5547 - val_loss: 0.5558\n",
      "Epoch 31/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 3ms/step - loss: 0.5549 - val_loss: 0.5558\n",
      "Epoch 32/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 3ms/step - loss: 0.5550 - val_loss: 0.5558\n",
      "Epoch 33/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 3ms/step - loss: 0.5536 - val_loss: 0.5147\n",
      "Epoch 34/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 3ms/step - loss: 0.5119 - val_loss: 0.5124\n",
      "Epoch 35/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 4ms/step - loss: 0.5122 - val_loss: 0.5116\n",
      "Epoch 36/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 3ms/step - loss: 0.5112 - val_loss: 0.5116\n",
      "Epoch 37/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 3ms/step - loss: 0.5094 - val_loss: 0.5121\n",
      "Epoch 38/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 3ms/step - loss: 0.5117 - val_loss: 0.5149\n",
      "Epoch 39/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 3ms/step - loss: 0.5123 - val_loss: 0.5121\n",
      "Epoch 40/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 3ms/step - loss: 0.5106 - val_loss: 0.5121\n",
      "Epoch 41/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 3ms/step - loss: 0.5123 - val_loss: 0.5242\n",
      "Epoch 42/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 3ms/step - loss: 0.5234 - val_loss: 0.5241\n",
      "Epoch 43/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 4ms/step - loss: 0.5200 - val_loss: 0.5111\n",
      "Epoch 44/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 3ms/step - loss: 0.5096 - val_loss: 0.5112\n",
      "Epoch 45/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 3ms/step - loss: 0.5106 - val_loss: 0.5111\n",
      "Epoch 46/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 3ms/step - loss: 0.5201 - val_loss: 0.5169\n",
      "Epoch 47/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 4ms/step - loss: 0.5185 - val_loss: 0.5263\n",
      "Epoch 48/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 4ms/step - loss: 0.5300 - val_loss: 0.5270\n",
      "Epoch 49/50\n",
      "\u001b[1m91645/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 4ms/step - loss: 0.5261 - val_loss: 0.5284\n",
      "Epoch 50/50\n",
      "\u001b[1m80520/91645\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m38s\u001b[0m 3ms/step - loss: 0.5260"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load the dataset (replace with your dataset path)\n",
    "data = pd.read_csv('../data/DoS_dataset.csv')\n",
    "\n",
    "# Step 1: Inspect the column names and data types\n",
    "print(\"Columns in the dataset:\", data.columns)\n",
    "print(data.dtypes)\n",
    "\n",
    "# Step 2: Handle non-numeric columns by applying Label Encoding\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':  # Check if the column is non-numeric\n",
    "        print(f\"Encoding column: {column}\")\n",
    "        label_encoder = LabelEncoder()\n",
    "        data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# Step 3: Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data.values)  # Since there are no labels, all data is features (X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Note: This split is still important even in unsupervised learning, to test the model on unseen data\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Build the autoencoder model for unsupervised anomaly detection\n",
    "def build_autoencoder(input_dim):\n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    encoder = layers.Dense(64, activation=\"relu\")(input_layer)\n",
    "    encoder = layers.Dense(32, activation=\"relu\")(encoder)\n",
    "    encoder = layers.Dense(16, activation=\"relu\")(encoder)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder = layers.Dense(32, activation=\"relu\")(encoder)\n",
    "    decoder = layers.Dense(64, activation=\"relu\")(decoder)\n",
    "    decoder = layers.Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
    "    \n",
    "    # Full model\n",
    "    autoencoder = models.Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "input_dim = X_train.shape[1]  # Number of features in the dataset\n",
    "autoencoder = build_autoencoder(input_dim)\n",
    "\n",
    "# Step 5: Train the model\n",
    "# In unsupervised learning, we train the autoencoder only on the features (X_train)\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Step 6: Anomaly Detection based on reconstruction loss\n",
    "def detect_anomalies(data, model, threshold=0.1):\n",
    "    reconstructions = model.predict(data)\n",
    "    reconstruction_loss = np.mean(np.abs(data - reconstructions), axis=1)\n",
    "    anomalies = reconstruction_loss > threshold\n",
    "    return anomalies, reconstruction_loss\n",
    "\n",
    "# Detect anomalies in the test data\n",
    "anomalies, reconstruction_loss = detect_anomalies(X_test, autoencoder)\n",
    "\n",
    "# Step 7: Display the detected anomalies and corresponding reconstruction loss\n",
    "print(\"Anomalies detected:\", anomalies)\n",
    "print(\"Reconstruction loss:\", reconstruction_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9436d-1bf0-48c5-a433-16faf45faa25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
