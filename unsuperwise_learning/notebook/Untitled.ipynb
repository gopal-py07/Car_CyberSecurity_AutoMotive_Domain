{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb60b30-9efe-4db6-ab62-5ff2e6d08fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T09:05:39.474745Z",
     "iopub.status.busy": "2024-09-18T09:05:39.474745Z",
     "iopub.status.idle": "2024-09-18T09:05:39.637003Z",
     "shell.execute_reply": "2024-09-18T09:05:39.634093Z",
     "shell.execute_reply.started": "2024-09-18T09:05:39.474745Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m dos_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDoS_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     14\u001b[0m fuzzy_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuzzy_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     15\u001b[0m gear_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgear_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_path' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420886b7-da0c-4878-a9cc-fdf97f80cfa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T09:10:16.791761Z",
     "iopub.status.busy": "2024-09-18T09:10:16.791761Z",
     "iopub.status.idle": "2024-09-18T10:56:40.814393Z",
     "shell.execute_reply": "2024-09-18T10:56:40.809540Z",
     "shell.execute_reply.started": "2024-09-18T09:10:16.791761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data Shape: (200000, 5951)\n",
      "Epoch 1/50, Loss: 0.0023746995721012354\n",
      "Epoch 2/50, Loss: 0.0020012937020510435\n",
      "Epoch 3/50, Loss: 0.0014616844709962606\n",
      "Epoch 4/50, Loss: 0.0013969638384878635\n",
      "Epoch 5/50, Loss: 0.0015908514615148306\n",
      "Epoch 6/50, Loss: 0.001697818166576326\n",
      "Epoch 7/50, Loss: 0.0015258279163390398\n",
      "Epoch 8/50, Loss: 0.0015907399356365204\n",
      "Epoch 9/50, Loss: 0.0017025051638484001\n",
      "Epoch 10/50, Loss: 0.001883962075226009\n",
      "Epoch 11/50, Loss: 0.0018172279233112931\n",
      "Epoch 12/50, Loss: 0.0015567620284855366\n",
      "Epoch 13/50, Loss: 0.0016923604998737574\n",
      "Epoch 14/50, Loss: 0.0022242595441639423\n",
      "Epoch 15/50, Loss: 0.0016540784854441881\n",
      "Epoch 16/50, Loss: 0.0018038831185549498\n",
      "Epoch 17/50, Loss: 0.001801273669116199\n",
      "Epoch 18/50, Loss: 0.0016077545005828142\n",
      "Epoch 19/50, Loss: 0.0018933367682620883\n",
      "Epoch 20/50, Loss: 0.0014081960543990135\n",
      "Epoch 21/50, Loss: 0.0018347245641052723\n",
      "Epoch 22/50, Loss: 0.0019409796223044395\n",
      "Epoch 23/50, Loss: 0.0019131222506985068\n",
      "Epoch 24/50, Loss: 0.001511976821348071\n",
      "Epoch 25/50, Loss: 0.0019213106716051698\n",
      "Epoch 26/50, Loss: 0.0016985770780593157\n",
      "Epoch 27/50, Loss: 0.0015945921186357737\n",
      "Epoch 28/50, Loss: 0.002261421177536249\n",
      "Epoch 29/50, Loss: 0.0018580806208774447\n",
      "Epoch 30/50, Loss: 0.001856951043009758\n",
      "Epoch 31/50, Loss: 0.00195247249212116\n",
      "Epoch 32/50, Loss: 0.001466858317144215\n",
      "Epoch 33/50, Loss: 0.0018636573804542422\n",
      "Epoch 34/50, Loss: 0.0015823936555534601\n",
      "Epoch 35/50, Loss: 0.0018650279380381107\n",
      "Epoch 36/50, Loss: 0.001953665865585208\n",
      "Epoch 37/50, Loss: 0.0017956482479348779\n",
      "Epoch 38/50, Loss: 0.0013204413698986173\n",
      "Epoch 39/50, Loss: 0.0015072908718138933\n",
      "Epoch 40/50, Loss: 0.0016001558396965265\n",
      "Epoch 41/50, Loss: 0.0021228922996670008\n",
      "Epoch 42/50, Loss: 0.0015119474846869707\n",
      "Epoch 43/50, Loss: 0.001516875927336514\n",
      "Epoch 44/50, Loss: 0.0021040027495473623\n",
      "Epoch 45/50, Loss: 0.0014824591344222426\n",
      "Epoch 46/50, Loss: 0.0015447847545146942\n",
      "Epoch 47/50, Loss: 0.0017196617554873228\n",
      "Epoch 48/50, Loss: 0.001504745683632791\n",
      "Epoch 49/50, Loss: 0.0015819622203707695\n",
      "Epoch 50/50, Loss: 0.0016776049742475152\n",
      "Number of anomalies detected: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=50000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=50000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=50000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=50000)\n",
    "\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4b3b7-ea83-42ea-81d5-c8bbe36633b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
