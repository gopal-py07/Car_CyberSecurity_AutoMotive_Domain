{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb49731e-b2a4-4993-8840-c0235e8c8f44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T11:19:22.484423Z",
     "iopub.status.busy": "2024-09-20T11:19:22.482911Z",
     "iopub.status.idle": "2024-09-20T11:21:02.150824Z",
     "shell.execute_reply": "2024-09-20T11:21:02.148289Z",
     "shell.execute_reply.started": "2024-09-20T11:19:22.484423Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 16:49:37,727 - INFO - Data loaded and concatenated. Total rows: 4000, Total columns: 32\n",
      "2024-09-20 16:49:38,089 - INFO - Data preprocessing complete. Shape after preprocessing: (4000, 1665)\n",
      "2024-09-20 16:49:38,147 - INFO - DataLoader created with batch size 64.\n",
      "2024-09-20 16:49:38,160 - INFO - Autoencoder initialized with input size 1665.\n",
      "2024-09-20 16:49:42,579 - INFO - Epoch 1/50, Loss: 0.008496733382344246\n",
      "2024-09-20 16:49:44,489 - INFO - Epoch 2/50, Loss: 0.009959322400391102\n",
      "2024-09-20 16:49:46,236 - INFO - Epoch 3/50, Loss: 0.008872109465301037\n",
      "2024-09-20 16:49:48,614 - INFO - Epoch 4/50, Loss: 0.00819880049675703\n",
      "2024-09-20 16:49:51,338 - INFO - Epoch 5/50, Loss: 0.007909242063760757\n",
      "2024-09-20 16:49:53,030 - INFO - Epoch 6/50, Loss: 0.007878273725509644\n",
      "2024-09-20 16:49:54,629 - INFO - Epoch 7/50, Loss: 0.006869259290397167\n",
      "2024-09-20 16:49:56,276 - INFO - Epoch 8/50, Loss: 0.007425455842167139\n",
      "2024-09-20 16:49:57,627 - INFO - Epoch 9/50, Loss: 0.010191013105213642\n",
      "2024-09-20 16:49:59,954 - INFO - Epoch 10/50, Loss: 0.00641030166298151\n",
      "2024-09-20 16:50:01,294 - INFO - Epoch 11/50, Loss: 0.007237259764224291\n",
      "2024-09-20 16:50:02,484 - INFO - Epoch 12/50, Loss: 0.006443972699344158\n",
      "2024-09-20 16:50:03,430 - INFO - Epoch 13/50, Loss: 0.006375272758305073\n",
      "2024-09-20 16:50:04,502 - INFO - Epoch 14/50, Loss: 0.008025554940104485\n",
      "2024-09-20 16:50:05,434 - INFO - Epoch 15/50, Loss: 0.008013549260795116\n",
      "2024-09-20 16:50:06,465 - INFO - Epoch 16/50, Loss: 0.005681195296347141\n",
      "2024-09-20 16:50:07,512 - INFO - Epoch 17/50, Loss: 0.007345711812376976\n",
      "2024-09-20 16:50:08,696 - INFO - Epoch 18/50, Loss: 0.007589559070765972\n",
      "2024-09-20 16:50:09,814 - INFO - Epoch 19/50, Loss: 0.005254250485450029\n",
      "2024-09-20 16:50:10,906 - INFO - Epoch 20/50, Loss: 0.008356994949281216\n",
      "2024-09-20 16:50:11,960 - INFO - Epoch 21/50, Loss: 0.006078516133129597\n",
      "2024-09-20 16:50:12,954 - INFO - Epoch 22/50, Loss: 0.006112346891313791\n",
      "2024-09-20 16:50:14,096 - INFO - Epoch 23/50, Loss: 0.008086424320936203\n",
      "2024-09-20 16:50:15,160 - INFO - Epoch 24/50, Loss: 0.007952140644192696\n",
      "2024-09-20 16:50:16,329 - INFO - Epoch 25/50, Loss: 0.005756123922765255\n",
      "2024-09-20 16:50:17,527 - INFO - Epoch 26/50, Loss: 0.006002143956720829\n",
      "2024-09-20 16:50:18,821 - INFO - Epoch 27/50, Loss: 0.007900563068687916\n",
      "2024-09-20 16:50:20,166 - INFO - Epoch 28/50, Loss: 0.006216443609446287\n",
      "2024-09-20 16:50:21,327 - INFO - Epoch 29/50, Loss: 0.00650465814396739\n",
      "2024-09-20 16:50:22,724 - INFO - Epoch 30/50, Loss: 0.0056359171867370605\n",
      "2024-09-20 16:50:24,205 - INFO - Epoch 31/50, Loss: 0.00719365244731307\n",
      "2024-09-20 16:50:25,746 - INFO - Epoch 32/50, Loss: 0.006254773586988449\n",
      "2024-09-20 16:50:27,244 - INFO - Epoch 33/50, Loss: 0.00855217594653368\n",
      "2024-09-20 16:50:28,925 - INFO - Epoch 34/50, Loss: 0.0056561133824288845\n",
      "2024-09-20 16:50:30,359 - INFO - Epoch 35/50, Loss: 0.005954832769930363\n",
      "2024-09-20 16:50:31,873 - INFO - Epoch 36/50, Loss: 0.005841651000082493\n",
      "2024-09-20 16:50:33,394 - INFO - Epoch 37/50, Loss: 0.005565360188484192\n",
      "2024-09-20 16:50:34,953 - INFO - Epoch 38/50, Loss: 0.005901137366890907\n",
      "2024-09-20 16:50:36,625 - INFO - Epoch 39/50, Loss: 0.005103559698909521\n",
      "2024-09-20 16:50:38,505 - INFO - Epoch 40/50, Loss: 0.0055810133926570415\n",
      "2024-09-20 16:50:40,159 - INFO - Epoch 41/50, Loss: 0.006891991943120956\n",
      "2024-09-20 16:50:41,942 - INFO - Epoch 42/50, Loss: 0.005582181271165609\n",
      "2024-09-20 16:50:43,719 - INFO - Epoch 43/50, Loss: 0.006258311215788126\n",
      "2024-09-20 16:50:45,601 - INFO - Epoch 44/50, Loss: 0.005651616025716066\n",
      "2024-09-20 16:50:47,669 - INFO - Epoch 45/50, Loss: 0.004862633533775806\n",
      "2024-09-20 16:50:49,578 - INFO - Epoch 46/50, Loss: 0.0054824440740048885\n",
      "2024-09-20 16:50:51,457 - INFO - Epoch 47/50, Loss: 0.007361778058111668\n",
      "2024-09-20 16:50:53,876 - INFO - Epoch 48/50, Loss: 0.005940788891166449\n",
      "2024-09-20 16:50:58,108 - INFO - Epoch 49/50, Loss: 0.004591643810272217\n",
      "2024-09-20 16:51:00,712 - INFO - Epoch 50/50, Loss: 0.005693540908396244\n",
      "2024-09-20 16:51:01,353 - INFO - Reconstruction errors calculated.\n",
      "2024-09-20 16:51:01,356 - INFO - Dynamically calculated threshold based on data: 0.007031583693434248\n",
      "2024-09-20 16:51:02,106 - INFO - Anomalies detected at rows (indices): [13, 19, 37, 43, 47]\n",
      "2024-09-20 16:51:02,107 - INFO - Anomaly values (reconstruction errors): [0.007195169106125832, 0.007422026712447405, 0.00728395814076066, 0.007373359519988298, 0.00749355461448431]\n",
      "2024-09-20 16:51:02,130 - INFO - Anomalies logged.\n",
      "2024-09-20 16:51:02,132 - WARNING - Driver Alert: Critical anomaly detected at row 13\n",
      "2024-09-20 16:51:02,135 - WARNING - Driver Alert: Critical anomaly detected at row 19\n",
      "2024-09-20 16:51:02,137 - WARNING - Driver Alert: Critical anomaly detected at row 37\n",
      "2024-09-20 16:51:02,139 - WARNING - Driver Alert: Critical anomaly detected at row 43\n",
      "2024-09-20 16:51:02,140 - WARNING - Driver Alert: Critical anomaly detected at row 47\n",
      "2024-09-20 16:51:02,142 - ERROR - ECU ECU_1 is isolated due to suspicious activity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anomalies detected: 5\n",
      "Anomalies detected at rows (indices): [13, 19, 37, 43, 47]\n",
      "Anomaly values (reconstruction errors): [0.007195169106125832, 0.007422026712447405, 0.00728395814076066, 0.007373359519988298, 0.00749355461448431]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# ==============================\n",
    "# Logging Configuration\n",
    "# ==============================\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Define the format\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"system_log.log\"),  # Save logs to a file\n",
    "        logging.StreamHandler()  # Also display logs on the console\n",
    "    ]\n",
    ")\n",
    "data_path = '../data/'\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=1000) \n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=1000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=1000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=1000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "logging.info(f\"Data loaded and concatenated. Total rows: {data.shape[0]}, Total columns: {data.shape[1]}\")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "logging.info(f\"Data preprocessing complete. Shape after preprocessing: {data_preprocessed.shape}\")\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "logging.info(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "logging.info(f\"Autoencoder initialized with input size {input_size}.\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Function to calculate reconstruction errors\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    logging.info(\"Reconstruction errors calculated.\")\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Step 2: Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Step 3: Dynamically calculate the threshold based on the mean and standard deviation of reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "\n",
    "# Set threshold as mean + 2 standard deviations\n",
    "threshold = mean_error + 2 * std_error\n",
    "logging.info(f\"Dynamically calculated threshold based on data: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies and get their corresponding row indices and reconstruction error values\n",
    "def detect_anomalies_with_values(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # List to store indices of the anomalies\n",
    "    anomaly_values = []  # List to store reconstruction errors (anomaly values)\n",
    "    idx = 0  # Index counter for the data rows\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_error = loss.item()  # Reconstruction error for this batch\n",
    "        \n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if reconstruction_error > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)  # Store the index of the anomaly\n",
    "            anomaly_values.append(reconstruction_error)  # Store the value of the anomaly (reconstruction error)\n",
    "        idx += 1  # Increment index counter\n",
    "    \n",
    "    logging.info(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n",
    "    logging.info(f\"Anomaly values (reconstruction errors): {anomaly_values}\")\n",
    "    return anomalies, anomaly_indices, anomaly_values\n",
    "\n",
    "# Detect anomalies, their indices, and values\n",
    "anomalies, anomaly_indices, anomaly_values = detect_anomalies_with_values(dataloader, model, threshold)\n",
    "\n",
    "# Print the number of anomalies detected, their corresponding row indices, and their reconstruction error values\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "print(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n",
    "print(f\"Anomaly values (reconstruction errors): {anomaly_values}\")\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Section with Logging\n",
    "# ==============================\n",
    "\n",
    "# A. Log Anomalies\n",
    "def log_anomalies(anomaly_indices, anomaly_values, data):\n",
    "    # Log anomalies with details\n",
    "    with open(\"anomaly_log.txt\", \"w\") as log_file:\n",
    "        for i, idx in enumerate(anomaly_indices):\n",
    "            log_file.write(f\"Anomaly at row {idx}, Error: {anomaly_values[i]}, Data: {data[idx]}\\n\")\n",
    "    logging.info(\"Anomalies logged.\")\n",
    "\n",
    "log_anomalies(anomaly_indices, anomaly_values, data_tensor)\n",
    "\n",
    "# B. Alert Driver if critical anomaly is detected\n",
    "def alert_driver(message):\n",
    "    logging.warning(f\"Driver Alert: {message}\")\n",
    "\n",
    "# If critical anomaly is detected (e.g., anomaly in sensor data, ECU data), alert driver\n",
    "for idx in anomaly_indices:\n",
    "    alert_driver(f\"Critical anomaly detected at row {idx}\")\n",
    "\n",
    "# C. Isolate the ECU if compromised (pseudo-code)\n",
    "def isolate_ecu(ecu_id):\n",
    "    logging.error(f\"ECU {ecu_id} is isolated due to suspicious activity.\")\n",
    "\n",
    "# Example: Isolate a specific ECU if anomaly is related to that ECU (replace 'ecu_id' with real data)\n",
    "isolate_ecu('ECU_1')\n",
    "\n",
    "# D. Trigger Safe Mode in case of critical failure\n",
    "def trigger_safe_mode():\n",
    "    logging.critical(\"Entering safe mode due to anomaly detection...\")\n",
    "\n",
    "# Trigger safe mode for severe anomalies\n",
    "if len(anomaly_indices) > 5:  # Example condition: more than 5 anomalies trigger safe mode\n",
    "    trigger_safe_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17acbbf-8e5d-464a-8784-36a2a57be1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
