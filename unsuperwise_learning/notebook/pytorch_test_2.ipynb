{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9704c522-b220-49cb-985e-f022eebaddb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T12:48:42.760883Z",
     "iopub.status.busy": "2024-09-18T12:48:42.760883Z",
     "iopub.status.idle": "2024-09-18T12:48:44.291809Z",
     "shell.execute_reply": "2024-09-18T12:48:44.291809Z",
     "shell.execute_reply.started": "2024-09-18T12:48:42.760883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 40000\n",
      "Number of batches in DataLoader: 625\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "from scipy import sparse\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing1.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting unsupervised learning data preprocessing.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=10000)\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=10000)\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=10000)\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=10000)\n",
    "    logging.info('Datasets loaded successfully with the first 1000 rows from each file.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataset = TensorDataset(X_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "logging.info(f'DataLoader created with batch size of 64. Total batches: {len(data_loader)}')\n",
    "\n",
    "# Print summary and log completion\n",
    "print(f\"Number of data points: {len(X_tensor)}\")\n",
    "print(f\"Number of batches in DataLoader: {len(data_loader)}\")\n",
    "logging.info('Data preprocessing completed successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d018f3e-1619-46f4-95e1-16f2d0c9052f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T12:48:45.416634Z",
     "iopub.status.busy": "2024-09-18T12:48:45.415632Z",
     "iopub.status.idle": "2024-09-18T13:09:24.134187Z",
     "shell.execute_reply": "2024-09-18T13:09:24.131178Z",
     "shell.execute_reply.started": "2024-09-18T12:48:45.416634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: nan\n",
      "Epoch [2/50], Loss: nan\n",
      "Epoch [3/50], Loss: nan\n",
      "Epoch [4/50], Loss: nan\n",
      "Epoch [5/50], Loss: nan\n",
      "Epoch [6/50], Loss: nan\n",
      "Epoch [7/50], Loss: nan\n",
      "Epoch [8/50], Loss: nan\n",
      "Epoch [9/50], Loss: nan\n",
      "Epoch [10/50], Loss: nan\n",
      "Epoch [11/50], Loss: nan\n",
      "Epoch [12/50], Loss: nan\n",
      "Epoch [13/50], Loss: nan\n",
      "Epoch [14/50], Loss: nan\n",
      "Epoch [15/50], Loss: nan\n",
      "Epoch [16/50], Loss: nan\n",
      "Epoch [17/50], Loss: nan\n",
      "Epoch [18/50], Loss: nan\n",
      "Epoch [19/50], Loss: nan\n",
      "Epoch [20/50], Loss: nan\n",
      "Epoch [21/50], Loss: nan\n",
      "Epoch [22/50], Loss: nan\n",
      "Epoch [23/50], Loss: nan\n",
      "Epoch [24/50], Loss: nan\n",
      "Epoch [25/50], Loss: nan\n",
      "Epoch [26/50], Loss: nan\n",
      "Epoch [27/50], Loss: nan\n",
      "Epoch [28/50], Loss: nan\n",
      "Epoch [29/50], Loss: nan\n",
      "Epoch [30/50], Loss: nan\n",
      "Epoch [31/50], Loss: nan\n",
      "Epoch [32/50], Loss: nan\n",
      "Epoch [33/50], Loss: nan\n",
      "Epoch [34/50], Loss: nan\n",
      "Epoch [35/50], Loss: nan\n",
      "Epoch [36/50], Loss: nan\n",
      "Epoch [37/50], Loss: nan\n",
      "Epoch [38/50], Loss: nan\n",
      "Epoch [39/50], Loss: nan\n",
      "Epoch [40/50], Loss: nan\n",
      "Epoch [41/50], Loss: nan\n",
      "Epoch [42/50], Loss: nan\n",
      "Epoch [43/50], Loss: nan\n",
      "Epoch [44/50], Loss: nan\n",
      "Epoch [45/50], Loss: nan\n",
      "Epoch [46/50], Loss: nan\n",
      "Epoch [47/50], Loss: nan\n",
      "Epoch [48/50], Loss: nan\n",
      "Epoch [49/50], Loss: nan\n",
      "Epoch [50/50], Loss: nan\n",
      "Number of anomalies detected: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing_anomaly_detection.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting data preprocessing for anomaly detection.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=20000)\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=20000)\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=20000)\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=20000)\n",
    "    logging.info('Datasets loaded successfully with the first 1000 rows from each file.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "logging.info('Numeric data normalized using StandardScaler.')\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "logging.info('Data concatenated and converted to dense format for PyTorch.')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Step 1: Train-Test Split\n",
    "X_train, X_test = train_test_split(X_tensor, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Data split into training and test sets with a test size of 20%.\")\n",
    "\n",
    "# Create DataLoaders for both training and test sets\n",
    "train_dataset = TensorDataset(X_train)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "logging.info('DataLoader created for both training and test sets.')\n",
    "\n",
    "# Step 2: Define the Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid to ensure outputs are between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "input_dim = X_tensor.shape[1]  # Number of input features\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "logging.info(f\"Autoencoder model initialized with input dimension: {input_dim}.\")\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "logging.info('Loss function and optimizer set up.')\n",
    "\n",
    "# Train the Autoencoder\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Compare output with input\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    logging.info(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "logging.info('Training complete.')\n",
    "\n",
    "# Step 4: Anomaly Detection\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    with torch.no_grad():  # No need to compute gradients in evaluation\n",
    "        for data in data_loader:\n",
    "            inputs = data[0]\n",
    "            outputs = model(inputs)\n",
    "            reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "            # If reconstruction error is above the threshold, flag as anomaly\n",
    "            anomalies += (reconstruction_error > threshold).cpu().numpy().tolist()\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = 0.02\n",
    "logging.info(f\"Anomaly detection threshold set to {threshold}.\")\n",
    "\n",
    "# Detect anomalies in the test set\n",
    "anomalies = detect_anomalies(test_loader, autoencoder, threshold)\n",
    "\n",
    "# Print the number of detected anomalies\n",
    "num_anomalies = sum(anomalies)\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "logging.info(f\"Number of anomalies detected: {num_anomalies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "738be20a-4f93-4ef3-9f07-97adda4095de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T11:22:10.799863Z",
     "iopub.status.busy": "2024-09-18T11:22:10.799863Z",
     "iopub.status.idle": "2024-09-18T11:22:38.122955Z",
     "shell.execute_reply": "2024-09-18T11:22:38.120947Z",
     "shell.execute_reply.started": "2024-09-18T11:22:10.799863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: nan\n",
      "Epoch [2/50], Loss: nan\n",
      "Epoch [3/50], Loss: nan\n",
      "Epoch [4/50], Loss: nan\n",
      "Epoch [5/50], Loss: nan\n",
      "Epoch [6/50], Loss: nan\n",
      "Epoch [7/50], Loss: nan\n",
      "Epoch [8/50], Loss: nan\n",
      "Epoch [9/50], Loss: nan\n",
      "Epoch [10/50], Loss: nan\n",
      "Epoch [11/50], Loss: nan\n",
      "Epoch [12/50], Loss: nan\n",
      "Epoch [13/50], Loss: nan\n",
      "Epoch [14/50], Loss: nan\n",
      "Epoch [15/50], Loss: nan\n",
      "Epoch [16/50], Loss: nan\n",
      "Epoch [17/50], Loss: nan\n",
      "Epoch [18/50], Loss: nan\n",
      "Epoch [19/50], Loss: nan\n",
      "Epoch [20/50], Loss: nan\n",
      "Epoch [21/50], Loss: nan\n",
      "Epoch [22/50], Loss: nan\n",
      "Epoch [23/50], Loss: nan\n",
      "Epoch [24/50], Loss: nan\n",
      "Epoch [25/50], Loss: nan\n",
      "Epoch [26/50], Loss: nan\n",
      "Epoch [27/50], Loss: nan\n",
      "Epoch [28/50], Loss: nan\n",
      "Epoch [29/50], Loss: nan\n",
      "Epoch [30/50], Loss: nan\n",
      "Epoch [31/50], Loss: nan\n",
      "Epoch [32/50], Loss: nan\n",
      "Epoch [33/50], Loss: nan\n",
      "Epoch [34/50], Loss: nan\n",
      "Epoch [35/50], Loss: nan\n",
      "Epoch [36/50], Loss: nan\n",
      "Epoch [37/50], Loss: nan\n",
      "Epoch [38/50], Loss: nan\n",
      "Epoch [39/50], Loss: nan\n",
      "Epoch [40/50], Loss: nan\n",
      "Epoch [41/50], Loss: nan\n",
      "Epoch [42/50], Loss: nan\n",
      "Epoch [43/50], Loss: nan\n",
      "Epoch [44/50], Loss: nan\n",
      "Epoch [45/50], Loss: nan\n",
      "Epoch [46/50], Loss: nan\n",
      "Epoch [47/50], Loss: nan\n",
      "Epoch [48/50], Loss: nan\n",
      "Epoch [49/50], Loss: nan\n",
      "Epoch [50/50], Loss: nan\n",
      "Number of anomalies detected: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11549\\AppData\\Local\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:6834: RuntimeWarning: All-NaN slice encountered\n",
      "  xmin = min(xmin, np.nanmin(xi))\n",
      "C:\\Users\\11549\\AppData\\Local\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:6835: RuntimeWarning: All-NaN slice encountered\n",
      "  xmax = max(xmax, np.nanmax(xi))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "autodetected range of [nan, nan] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 197\u001b[0m\n\u001b[0;32m    194\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Call the function to plot errors\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m plot_reconstruction_errors(test_loader, autoencoder)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Tuning the threshold based on the results\u001b[39;00m\n\u001b[0;32m    200\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m  \u001b[38;5;66;03m# Adjust this if you're missing anomalies or detecting too many\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 190\u001b[0m, in \u001b[0;36mplot_reconstruction_errors\u001b[1;34m(data_loader, model)\u001b[0m\n\u001b[0;32m    187\u001b[0m         reconstruction_errors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reconstruction_error\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Plot histogram of reconstruction errors\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(reconstruction_errors, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    191\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReconstruction Error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    192\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Data Points\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:3236\u001b[0m, in \u001b[0;36mhist\u001b[1;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mhist)\n\u001b[0;32m   3212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhist\u001b[39m(\n\u001b[0;32m   3213\u001b[0m     x: ArrayLike \u001b[38;5;241m|\u001b[39m Sequence[ArrayLike],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3234\u001b[0m     BarContainer \u001b[38;5;241m|\u001b[39m Polygon \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[BarContainer \u001b[38;5;241m|\u001b[39m Polygon],\n\u001b[0;32m   3235\u001b[0m ]:\n\u001b[1;32m-> 3236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mhist(\n\u001b[0;32m   3237\u001b[0m         x,\n\u001b[0;32m   3238\u001b[0m         bins\u001b[38;5;241m=\u001b[39mbins,\n\u001b[0;32m   3239\u001b[0m         \u001b[38;5;28mrange\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m,\n\u001b[0;32m   3240\u001b[0m         density\u001b[38;5;241m=\u001b[39mdensity,\n\u001b[0;32m   3241\u001b[0m         weights\u001b[38;5;241m=\u001b[39mweights,\n\u001b[0;32m   3242\u001b[0m         cumulative\u001b[38;5;241m=\u001b[39mcumulative,\n\u001b[0;32m   3243\u001b[0m         bottom\u001b[38;5;241m=\u001b[39mbottom,\n\u001b[0;32m   3244\u001b[0m         histtype\u001b[38;5;241m=\u001b[39mhisttype,\n\u001b[0;32m   3245\u001b[0m         align\u001b[38;5;241m=\u001b[39malign,\n\u001b[0;32m   3246\u001b[0m         orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   3247\u001b[0m         rwidth\u001b[38;5;241m=\u001b[39mrwidth,\n\u001b[0;32m   3248\u001b[0m         log\u001b[38;5;241m=\u001b[39mlog,\n\u001b[0;32m   3249\u001b[0m         color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m   3250\u001b[0m         label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[0;32m   3251\u001b[0m         stacked\u001b[38;5;241m=\u001b[39mstacked,\n\u001b[0;32m   3252\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3253\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3254\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:6862\u001b[0m, in \u001b[0;36mAxes.hist\u001b[1;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[0;32m   6858\u001b[0m \u001b[38;5;66;03m# Loop through datasets\u001b[39;00m\n\u001b[0;32m   6859\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nx):\n\u001b[0;32m   6860\u001b[0m     \u001b[38;5;66;03m# this will automatically overwrite bins,\u001b[39;00m\n\u001b[0;32m   6861\u001b[0m     \u001b[38;5;66;03m# so that each histogram uses the same bins\u001b[39;00m\n\u001b[1;32m-> 6862\u001b[0m     m, bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhistogram(x[i], bins, weights\u001b[38;5;241m=\u001b[39mw[i], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhist_kwargs)\n\u001b[0;32m   6863\u001b[0m     tops\u001b[38;5;241m.\u001b[39mappend(m)\n\u001b[0;32m   6864\u001b[0m tops \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tops, \u001b[38;5;28mfloat\u001b[39m)  \u001b[38;5;66;03m# causes problems later if it's an int\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\lib\\histograms.py:780\u001b[0m, in \u001b[0;36mhistogram\u001b[1;34m(a, bins, range, density, weights)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    776\u001b[0m \n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    778\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[1;32m--> 780\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m _get_bin_edges(a, bins, \u001b[38;5;28mrange\u001b[39m, weights)\n\u001b[0;32m    782\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\lib\\histograms.py:426\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[1;34m(a, bins, range, weights)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_equal_bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m _get_outer_edges(a, \u001b[38;5;28mrange\u001b[39m)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(bins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    429\u001b[0m     bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(bins)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\lib\\histograms.py:323\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[1;34m(a, range)\u001b[0m\n\u001b[0;32m    321\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mmin(), a\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(first_edge) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(last_edge)):\n\u001b[1;32m--> 323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautodetected range of [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] is not finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(first_edge, last_edge))\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# expand empty range to avoid divide by zero\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_edge \u001b[38;5;241m==\u001b[39m last_edge:\n",
      "\u001b[1;31mValueError\u001b[0m: autodetected range of [nan, nan] is not finite"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcu0lEQVR4nO3db2yV5f348U9paaturRG0FkEEpxMl6mgDo6wandag0ZBskcVF1GliszmETqeMRYYxaXTRfXUKbgoaE3REReeDztEHG1Zxf2DFGCFxEWZBW0kxtqhbGXD/Hhj6W9fiOLV/uNrXK7kfnMv7Puc6uazn7X2fP3lZlmUBAJCAMcM9AQCAIyVcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGTkHC6vvPJKXHnllTFhwoTIy8uLF1988X8es2HDhqioqIji4uKYOnVqPProo/2ZKwAwyuUcLp988kmcd9558fDDDx/R/jt27IjLL788qquro7m5OX7yk5/EwoUL4/nnn895sgDA6Jb3RX5kMS8vL1544YWYN2/eYfe544474qWXXopt27Z1j9XW1sYbb7wRr7/+en8fGgAYhQoG+wFef/31qKmp6TF22WWXxapVq+Lf//53jB07ttcxXV1d0dXV1X374MGD8eGHH8a4ceMiLy9vsKcMAAyALMti7969MWHChBgzZmDeVjvo4dLW1hZlZWU9xsrKymL//v3R3t4e5eXlvY6pr6+P5cuXD/bUAIAhsHPnzpg4ceKA3Negh0tE9DpLcujq1OHOnixZsiTq6uq6b3d0dMSpp54aO3fujJKSksGbKAAwYDo7O2PSpEnx5S9/ecDuc9DD5eSTT462trYeY7t3746CgoIYN25cn8cUFRVFUVFRr/GSkhLhAgCJGci3eQz697jMnj07Ghsbe4ytX78+Kisr+3x/CwDA4eQcLh9//HFs2bIltmzZEhGffdx5y5Yt0dLSEhGfXeZZsGBB9/61tbXx7rvvRl1dXWzbti1Wr14dq1atittuu21gngEAMGrkfKlo06ZNcdFFF3XfPvRelOuuuy6efPLJaG1t7Y6YiIgpU6ZEQ0NDLF68OB555JGYMGFCPPTQQ/Gtb31rAKYPAIwmX+h7XIZKZ2dnlJaWRkdHh/e4AEAiBuP1228VAQDJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjH6Fy4oVK2LKlClRXFwcFRUV0dTU9Ln7r1mzJs4777w49thjo7y8PG644YbYs2dPvyYMAIxeOYfL2rVrY9GiRbF06dJobm6O6urqmDt3brS0tPS5/6uvvhoLFiyIG2+8Md5666149tln469//WvcdNNNX3jyAMDoknO4PPDAA3HjjTfGTTfdFNOmTYv/+7//i0mTJsXKlSv73P9Pf/pTnHbaabFw4cKYMmVKfOMb34ibb745Nm3a9IUnDwCMLjmFy759+2Lz5s1RU1PTY7ympiY2btzY5zFVVVWxa9euaGhoiCzL4oMPPojnnnsurrjiisM+TldXV3R2dvbYAAByCpf29vY4cOBAlJWV9RgvKyuLtra2Po+pqqqKNWvWxPz586OwsDBOPvnkOP744+OXv/zlYR+nvr4+SktLu7dJkyblMk0AYITq15tz8/LyetzOsqzX2CFbt26NhQsXxl133RWbN2+Ol19+OXbs2BG1tbWHvf8lS5ZER0dH97Zz587+TBMAGGEKctl5/PjxkZ+f3+vsyu7du3udhTmkvr4+5syZE7fffntERJx77rlx3HHHRXV1ddxzzz1RXl7e65iioqIoKirKZWoAwCiQ0xmXwsLCqKioiMbGxh7jjY2NUVVV1ecxn376aYwZ0/Nh8vPzI+KzMzUAAEcq50tFdXV18fjjj8fq1atj27ZtsXjx4mhpaem+9LNkyZJYsGBB9/5XXnllrFu3LlauXBnbt2+P1157LRYuXBgzZ86MCRMmDNwzAQBGvJwuFUVEzJ8/P/bs2RN33313tLa2xvTp06OhoSEmT54cERGtra09vtPl+uuvj71798bDDz8cP/rRj+L444+Piy++OO69996BexYAwKiQlyVwvaazszNKS0ujo6MjSkpKhns6AMARGIzXb79VBAAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMvoVLitWrIgpU6ZEcXFxVFRURFNT0+fu39XVFUuXLo3JkydHUVFRnH766bF69ep+TRgAGL0Kcj1g7dq1sWjRolixYkXMmTMnfvWrX8XcuXNj69atceqpp/Z5zNVXXx0ffPBBrFq1Kr7yla/E7t27Y//+/V948gDA6JKXZVmWywGzZs2KGTNmxMqVK7vHpk2bFvPmzYv6+vpe+7/88svxne98J7Zv3x4nnHBCvybZ2dkZpaWl0dHRESUlJf26DwBgaA3G63dOl4r27dsXmzdvjpqamh7jNTU1sXHjxj6Peemll6KysjLuu+++OOWUU+LMM8+M2267Lf75z38e9nG6urqis7OzxwYAkNOlovb29jhw4ECUlZX1GC8rK4u2trY+j9m+fXu8+uqrUVxcHC+88EK0t7fH97///fjwww8P+z6X+vr6WL58eS5TAwBGgX69OTcvL6/H7SzLeo0dcvDgwcjLy4s1a9bEzJkz4/LLL48HHnggnnzyycOedVmyZEl0dHR0bzt37uzPNAGAESanMy7jx4+P/Pz8XmdXdu/e3esszCHl5eVxyimnRGlpaffYtGnTIsuy2LVrV5xxxhm9jikqKoqioqJcpgYAjAI5nXEpLCyMioqKaGxs7DHe2NgYVVVVfR4zZ86ceP/99+Pjjz/uHnv77bdjzJgxMXHixH5MGQAYrXK+VFRXVxePP/54rF69OrZt2xaLFy+OlpaWqK2tjYjPLvMsWLCge/9rrrkmxo0bFzfccENs3bo1Xnnllbj99tvje9/7XhxzzDED90wAgBEv5+9xmT9/fuzZsyfuvvvuaG1tjenTp0dDQ0NMnjw5IiJaW1ujpaWle/8vfelL0djYGD/84Q+jsrIyxo0bF1dffXXcc889A/csAIBRIefvcRkOvscFANIz7N/jAgAwnIQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJKNf4bJixYqYMmVKFBcXR0VFRTQ1NR3Rca+99loUFBTE+eef35+HBQBGuZzDZe3atbFo0aJYunRpNDc3R3V1dcydOzdaWlo+97iOjo5YsGBBfPOb3+z3ZAGA0S0vy7IslwNmzZoVM2bMiJUrV3aPTZs2LebNmxf19fWHPe473/lOnHHGGZGfnx8vvvhibNmy5bD7dnV1RVdXV/ftzs7OmDRpUnR0dERJSUku0wUAhklnZ2eUlpYO6Ot3Tmdc9u3bF5s3b46ampoe4zU1NbFx48bDHvfEE0/EO++8E8uWLTuix6mvr4/S0tLubdKkSblMEwAYoXIKl/b29jhw4ECUlZX1GC8rK4u2trY+j/n73/8ed955Z6xZsyYKCgqO6HGWLFkSHR0d3dvOnTtzmSYAMEIdWUn8l7y8vB63syzrNRYRceDAgbjmmmti+fLlceaZZx7x/RcVFUVRUVF/pgYAjGA5hcv48eMjPz+/19mV3bt39zoLExGxd+/e2LRpUzQ3N8ctt9wSEREHDx6MLMuioKAg1q9fHxdffPEXmD4AMJrkdKmosLAwKioqorGxscd4Y2NjVFVV9dq/pKQk3nzzzdiyZUv3VltbG1/96ldjy5YtMWvWrC82ewBgVMn5UlFdXV1ce+21UVlZGbNnz45f//rX0dLSErW1tRHx2ftT3nvvvXjqqadizJgxMX369B7Hn3TSSVFcXNxrHADgf8k5XObPnx979uyJu+++O1pbW2P69OnR0NAQkydPjoiI1tbW//mdLgAA/ZHz97gMh8H4HDgAMLiG/XtcAACGk3ABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZPQrXFasWBFTpkyJ4uLiqKioiKampsPuu27durj00kvjxBNPjJKSkpg9e3b8/ve/7/eEAYDRK+dwWbt2bSxatCiWLl0azc3NUV1dHXPnzo2WlpY+93/llVfi0ksvjYaGhti8eXNcdNFFceWVV0Zzc/MXnjwAMLrkZVmW5XLArFmzYsaMGbFy5crusWnTpsW8efOivr7+iO7jnHPOifnz58ddd93V5z/v6uqKrq6u7tudnZ0xadKk6OjoiJKSklymCwAMk87OzigtLR3Q1++czrjs27cvNm/eHDU1NT3Ga2pqYuPGjUd0HwcPHoy9e/fGCSeccNh96uvro7S0tHubNGlSLtMEAEaonMKlvb09Dhw4EGVlZT3Gy8rKoq2t7Yju4/77749PPvkkrr766sPus2TJkujo6Ojedu7cmcs0AYARqqA/B+Xl5fW4nWVZr7G+PPPMM/Gzn/0sfvvb38ZJJ5102P2KioqiqKioP1MDAEawnMJl/PjxkZ+f3+vsyu7du3udhflva9eujRtvvDGeffbZuOSSS3KfKQAw6uV0qaiwsDAqKiqisbGxx3hjY2NUVVUd9rhnnnkmrr/++nj66afjiiuu6N9MAYBRL+dLRXV1dXHttddGZWVlzJ49O379619HS0tL1NbWRsRn709577334qmnnoqIz6JlwYIF8eCDD8bXv/717rM1xxxzTJSWlg7gUwEARrqcw2X+/PmxZ8+euPvuu6O1tTWmT58eDQ0NMXny5IiIaG1t7fGdLr/61a9i//798YMf/CB+8IMfdI9fd9118eSTT37xZwAAjBo5f4/LcBiMz4EDAINr2L/HBQBgOAkXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASEa/wmXFihUxZcqUKC4ujoqKimhqavrc/Tds2BAVFRVRXFwcU6dOjUcffbRfkwUARrecw2Xt2rWxaNGiWLp0aTQ3N0d1dXXMnTs3Wlpa+tx/x44dcfnll0d1dXU0NzfHT37yk1i4cGE8//zzX3jyAMDokpdlWZbLAbNmzYoZM2bEypUru8emTZsW8+bNi/r6+l7733HHHfHSSy/Ftm3busdqa2vjjTfeiNdff73Px+jq6oqurq7u2x0dHXHqqafGzp07o6SkJJfpAgDDpLOzMyZNmhQfffRRlJaWDsydZjno6urK8vPzs3Xr1vUYX7hwYXbBBRf0eUx1dXW2cOHCHmPr1q3LCgoKsn379vV5zLJly7KIsNlsNpvNNgK2d955J5fc+FwFkYP29vY4cOBAlJWV9RgvKyuLtra2Po9pa2vrc//9+/dHe3t7lJeX9zpmyZIlUVdX1337o48+ismTJ0dLS8vAFRv9cqienf0aftbi6GEtji7W4+hx6IrJCSecMGD3mVO4HJKXl9fjdpZlvcb+1/59jR9SVFQURUVFvcZLS0v9S3iUKCkpsRZHCWtx9LAWRxfrcfQYM2bgPsSc0z2NHz8+8vPze51d2b17d6+zKoecfPLJfe5fUFAQ48aNy3G6AMBollO4FBYWRkVFRTQ2NvYYb2xsjKqqqj6PmT17dq/9169fH5WVlTF27NgcpwsAjGY5n7upq6uLxx9/PFavXh3btm2LxYsXR0tLS9TW1kbEZ+9PWbBgQff+tbW18e6770ZdXV1s27YtVq9eHatWrYrbbrvtiB+zqKgoli1b1uflI4aWtTh6WIujh7U4uliPo8dgrEXOH4eO+OwL6O67775obW2N6dOnxy9+8Yu44IILIiLi+uuvj3/84x/xxz/+sXv/DRs2xOLFi+Ott96KCRMmxB133NEdOgAAR6pf4QIAMBz8VhEAkAzhAgAkQ7gAAMkQLgBAMo6acFmxYkVMmTIliouLo6KiIpqamj53/w0bNkRFRUUUFxfH1KlT49FHHx2imY58uazFunXr4tJLL40TTzwxSkpKYvbs2fH73/9+CGc7suX6d3HIa6+9FgUFBXH++ecP7gRHkVzXoqurK5YuXRqTJ0+OoqKiOP3002P16tVDNNuRLde1WLNmTZx33nlx7LHHRnl5edxwww2xZ8+eIZrtyPXKK6/ElVdeGRMmTIi8vLx48cUX/+cxA/LaPWC/evQF/OY3v8nGjh2bPfbYY9nWrVuzW2+9NTvuuOOyd999t8/9t2/fnh177LHZrbfemm3dujV77LHHsrFjx2bPPffcEM985Ml1LW699dbs3nvvzf7yl79kb7/9drZkyZJs7Nix2d/+9rchnvnIk+taHPLRRx9lU6dOzWpqarLzzjtvaCY7wvVnLa666qps1qxZWWNjY7Zjx47sz3/+c/baa68N4axHplzXoqmpKRszZkz24IMPZtu3b8+ampqyc845J5s3b94Qz3zkaWhoyJYuXZo9//zzWURkL7zwwufuP1Cv3UdFuMycOTOrra3tMXbWWWdld955Z5/7//jHP87OOuusHmM333xz9vWvf33Q5jha5LoWfTn77LOz5cuXD/TURp3+rsX8+fOzn/70p9myZcuEywDJdS1+97vfZaWlpdmePXuGYnqjSq5r8fOf/zybOnVqj7GHHnoomzhx4qDNcTQ6knAZqNfuYb9UtG/fvti8eXPU1NT0GK+pqYmNGzf2eczrr7/ea//LLrssNm3aFP/+978Hba4jXX/W4r8dPHgw9u7dO6C/BDoa9XctnnjiiXjnnXdi2bJlgz3FUaM/a/HSSy9FZWVl3HfffXHKKafEmWeeGbfddlv885//HIopj1j9WYuqqqrYtWtXNDQ0RJZl8cEHH8Rzzz0XV1xxxVBMmf8wUK/d/fp16IHU3t4eBw4c6PUjjWVlZb1+nPGQtra2Pvffv39/tLe3R3l5+aDNdyTrz1r8t/vvvz8++eSTuPrqqwdjiqNGf9bi73//e9x5553R1NQUBQXD/qc9YvRnLbZv3x6vvvpqFBcXxwsvvBDt7e3x/e9/Pz788EPvc/kC+rMWVVVVsWbNmpg/f37861//iv3798dVV10Vv/zlL4diyvyHgXrtHvYzLofk5eX1uJ1lWa+x/7V/X+PkLte1OOSZZ56Jn/3sZ7F27do46aSTBmt6o8qRrsWBAwfimmuuieXLl8eZZ545VNMbVXL5uzh48GDk5eXFmjVrYubMmXH55ZfHAw88EE8++aSzLgMgl7XYunVrLFy4MO66667YvHlzvPzyy7Fjxw4/OzNMBuK1e9j/t2z8+PGRn5/fq5Z3797dq8wOOfnkk/vcv6CgIMaNGzdocx3p+rMWh6xduzZuvPHGePbZZ+OSSy4ZzGmOCrmuxd69e2PTpk3R3Nwct9xyS0R89uKZZVkUFBTE+vXr4+KLLx6SuY80/fm7KC8vj1NOOSVKS0u7x6ZNmxZZlsWuXbvijDPOGNQ5j1T9WYv6+vqYM2dO3H777RERce6558Zxxx0X1dXVcc899zhDP4QG6rV72M+4FBYWRkVFRTQ2NvYYb2xsjKqqqj6PmT17dq/9169fH5WVlTF27NhBm+tI15+1iPjsTMv1118fTz/9tOvGAyTXtSgpKYk333wztmzZ0r3V1tbGV7/61diyZUvMmjVrqKY+4vTn72LOnDnx/vvvx8cff9w99vbbb8eYMWNi4sSJgzrfkaw/a/Hpp5/GmDE9X+ry8/Mj4v//3z5DY8Beu3N6K+8gOfTxtlWrVmVbt27NFi1alB133HHZP/7xjyzLsuzOO+/Mrr322u79D32kavHixdnWrVuzVatW+Tj0AMl1LZ5++umsoKAge+SRR7LW1tbu7aOPPhqupzBi5LoW/82nigZOrmuxd+/ebOLEidm3v/3t7K233so2bNiQnXHGGdlNN900XE9hxMh1LZ544omsoKAgW7FiRfbOO+9kr776alZZWZnNnDlzuJ7CiLF3796subk5a25uziIie+CBB7Lm5ubuj6YP1mv3UREuWZZljzzySDZ58uSssLAwmzFjRrZhw4buf3bddddlF154YY/9//jHP2Zf+9rXssLCwuy0007LVq5cOcQzHrlyWYsLL7wwi4he23XXXTf0Ex+Bcv27+E/CZWDluhbbtm3LLrnkkuyYY47JJk6cmNXV1WWffvrpEM96ZMp1LR566KHs7LPPzo455pisvLw8++53v5vt2rVriGc98vzhD3/43P/+D9Zrd16WOVcGAKRh2N/jAgBwpIQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAk4/8BrQWhjBP+6s8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing_anomaly_detection.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting data preprocessing for anomaly detection.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=1000)\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=1000)\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=1000)\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=1000)\n",
    "    logging.info('Datasets loaded successfully with the first 1000 rows from each file.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "logging.info('Numeric data normalized using StandardScaler.')\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "logging.info('Data concatenated and converted to dense format for PyTorch.')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Step 1: Train-Test Split\n",
    "X_train, X_test = train_test_split(X_tensor, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Data split into training and test sets with a test size of 20%.\")\n",
    "\n",
    "# Create DataLoaders for both training and test sets\n",
    "train_dataset = TensorDataset(X_train)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "logging.info('DataLoader created for both training and test sets.')\n",
    "\n",
    "# Step 2: Define the Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid to ensure outputs are between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "input_dim = X_tensor.shape[1]  # Number of input features\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "logging.info(f\"Autoencoder model initialized with input dimension: {input_dim}.\")\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "logging.info('Loss function and optimizer set up.')\n",
    "\n",
    "# Train the Autoencoder\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Compare output with input\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    logging.info(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "logging.info('Training complete.')\n",
    "\n",
    "# Step 4: Anomaly Detection\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    with torch.no_grad():  # No need to compute gradients in evaluation\n",
    "        for data in data_loader:\n",
    "            inputs = data[0]\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute reconstruction error (MSE between inputs and outputs)\n",
    "            reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "            \n",
    "            # Flag as anomaly if reconstruction error exceeds the threshold\n",
    "            anomalies += (reconstruction_error > threshold).cpu().numpy().tolist()\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = 0.02  # Adjust this value based on your data and testing\n",
    "logging.info(f\"Anomaly detection threshold set to {threshold}.\")\n",
    "\n",
    "# Detect anomalies in the test set\n",
    "anomalies = detect_anomalies(test_loader, autoencoder, threshold)\n",
    "num_anomalies = sum(anomalies)\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "logging.info(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "\n",
    "# Plot reconstruction errors (optional for better visualization)\n",
    "def plot_reconstruction_errors(data_loader, model):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs = data[0]\n",
    "            outputs = model(inputs)\n",
    "            reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "            reconstruction_errors += reconstruction_error.cpu().numpy().tolist()\n",
    "    \n",
    "    # Plot histogram of reconstruction errors\n",
    "    plt.hist(reconstruction_errors, bins=50)\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Number of Data Points')\n",
    "    plt.title('Reconstruction Errors of Test Data')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot errors\n",
    "plot_reconstruction_errors(test_loader, autoencoder)\n",
    "\n",
    "# Tuning the threshold based on the results\n",
    "threshold = 0.01  # Adjust this if you're missing anomalies or detecting too many\n",
    "anomalies = detect_anomalies(test_loader, autoencoder, threshold)\n",
    "print(f\"Number of anomalies detected after tuning: {sum(anomalies)}\")\n",
    "logging.info(f\"Number of anomalies detected after tuning: {sum(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b291efb7-a991-41bb-8c3f-4fd3998379c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T13:09:24.771975Z",
     "iopub.status.busy": "2024-09-18T13:09:24.770976Z",
     "iopub.status.idle": "2024-09-18T13:09:30.228759Z",
     "shell.execute_reply": "2024-09-18T13:09:30.220373Z",
     "shell.execute_reply.started": "2024-09-18T13:09:24.771975Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.06 GiB for an array with shape (204000, 5960) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m combined_data_encoded \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mhstack([numeric_data_scaled_sparse, categorical_encoded])\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m X_dense \u001b[38;5;241m=\u001b[39m combined_data_encoded\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     90\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData concatenated and converted to dense format for PyTorch.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1106\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1106\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1327\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.06 GiB for an array with shape (204000, 5960) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to set up logging\n",
    "def setup_logging(debug=False):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    file_handler = logging.FileHandler('../logs/data_preprocessing_anomaly_detection.log')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    if debug:\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "    else:\n",
    "        logger.info(\"Console logging is disabled; only logging to file.\")\n",
    "\n",
    "setup_logging(debug=False)  # Set to True to enable console logging, False for file only\n",
    "\n",
    "logging.info('Starting data preprocessing for anomaly detection.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=51000)\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=51000)\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=51000)\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=51000)\n",
    "    logging.info('Datasets loaded successfully with the first 1000 rows from each file.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "logging.info('Numeric data normalized using StandardScaler.')\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "logging.info('Data concatenated and converted to dense format for PyTorch.')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Step 1: Train-Test Split\n",
    "X_train, X_test = train_test_split(X_tensor, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Data split into training and test sets with a test size of 20%.\")\n",
    "\n",
    "# Create DataLoaders for both training and test sets\n",
    "train_dataset = TensorDataset(X_train)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "logging.info('DataLoader created for both training and test sets.')\n",
    "\n",
    "# Step 2: Define the Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid to ensure outputs are between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "input_dim = X_tensor.shape[1]  # Number of input features\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "logging.info(f\"Autoencoder model initialized with input dimension: {input_dim}.\")\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "logging.info('Loss function and optimizer set up.')\n",
    "\n",
    "# Train the Autoencoder\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Compare output with input\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    logging.info(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "logging.info('Training complete.')\n",
    "\n",
    "# Step 4: Anomaly Detection\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()\n",
    "    anomalies = []\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs = data[0]\n",
    "            outputs = model(inputs)\n",
    "            reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "            anomalies += (reconstruction_error > threshold).cpu().numpy().tolist()\n",
    "            reconstruction_errors += reconstruction_error.cpu().numpy().tolist()\n",
    "    \n",
    "    return anomalies, reconstruction_errors\n",
    "\n",
    "# Updated function to handle NaN values in reconstruction errors\n",
    "def plot_reconstruction_errors(data_loader, model):\n",
    "    model.eval()\n",
    "    _, reconstruction_errors = detect_anomalies(data_loader, model, threshold=0.02)\n",
    "    reconstruction_errors = np.array(reconstruction_errors)\n",
    "    \n",
    "    # Remove NaN values before plotting\n",
    "    clean_reconstruction_errors = reconstruction_errors[~np.isnan(reconstruction_errors)]\n",
    "    \n",
    "    if len(clean_reconstruction_errors) == 0:\n",
    "        print(\"No valid reconstruction errors to plot.\")\n",
    "        logging.warning(\"All reconstruction errors were NaN.\")\n",
    "        return\n",
    "    \n",
    "    # Plot histogram of reconstruction errors\n",
    "    plt.hist(clean_reconstruction_errors, bins=50)\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Number of Data Points')\n",
    "    plt.title('Reconstruction Errors of Test Data')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot errors after handling NaN values\n",
    "plot_reconstruction_errors(test_loader, autoencoder)\n",
    "\n",
    "# Tuning the threshold based on the results\n",
    "threshold = 0.01  # Adjust this if you're missing anomalies or detecting too many\n",
    "anomalies = detect_anomalies(test_loader, autoencoder, threshold)[0]\n",
    "print(f\"Number of anomalies detected after tuning: {sum(anomalies)}\")\n",
    "logging.info(f\"Number of anomalies detected after tuning: {sum(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba52a78-7c88-4e70-9ea9-beef27f95b37",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-18T11:22:38.124952Z",
     "iopub.status.idle": "2024-09-18T11:22:38.124952Z",
     "shell.execute_reply": "2024-09-18T11:22:38.124952Z",
     "shell.execute_reply.started": "2024-09-18T11:22:38.124952Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=51000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=51000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=51000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=51000)\n",
    "print(\"data loade\")\n",
    "\n",
    "# Concatenate datasets for training\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "\n",
    "# Data preprocessing: Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)  # Normalize the data\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data_tensor = torch.tensor(data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_scaled.shape[1]  # Number of features (columns)\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf495fc8-2272-405e-9998-835d010a450f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-18T13:09:30.230767Z",
     "iopub.status.idle": "2024-09-18T13:09:30.231767Z",
     "shell.execute_reply": "2024-09-18T13:09:30.230767Z",
     "shell.execute_reply.started": "2024-09-18T13:09:30.230767Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=51000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=51000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=51000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=51000)\n",
    "print(\"data loade\")\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed.toarray(), dtype=torch.float32) if isinstance(data_preprocessed, pd.DataFrame) else torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06337b1-dd03-418b-9371-a19993fb6aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef08ed5-f6b7-4db3-951b-ef8dcf96537c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-18T13:09:30.238275Z",
     "iopub.status.idle": "2024-09-18T13:09:30.239838Z",
     "shell.execute_reply": "2024-09-18T13:09:30.239289Z",
     "shell.execute_reply.started": "2024-09-18T13:09:30.239289Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=20000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=20000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=20000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=20000)\n",
    "print(\"data loade\")\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908915cb-551d-45c4-9a1c-f17bb08565c9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-18T13:09:30.242855Z",
     "iopub.status.idle": "2024-09-18T13:09:30.244223Z",
     "shell.execute_reply": "2024-09-18T13:09:30.244223Z",
     "shell.execute_reply.started": "2024-09-18T13:09:30.244223Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=1000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=1000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=1000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=1000)\n",
    "print(\"data loade\")\n",
    "1\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7538cd-c9f0-4946-8332-218cf7b4a9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee1fcd-f80b-4abd-9e5e-1bc85deb8ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a782aeb-4417-4cb9-8a04-69a24f64d1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbaa977-b08b-40b9-b5e4-dcdbd1eef044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3624638-16d1-495f-ae95-4d296287cf59",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-18T13:09:30.262361Z",
     "iopub.status.idle": "2024-09-18T13:09:30.262361Z",
     "shell.execute_reply": "2024-09-18T13:09:30.262361Z",
     "shell.execute_reply.started": "2024-09-18T13:09:30.262361Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=50000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=50000)\n",
    "# gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=50000)\n",
    "# rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=50000)\n",
    "\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110cecd8-5a2c-4cb4-b7f5-5ed98c32730b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-18T13:09:30.262361Z",
     "iopub.status.idle": "2024-09-18T13:09:30.262361Z",
     "shell.execute_reply": "2024-09-18T13:09:30.262361Z",
     "shell.execute_reply.started": "2024-09-18T13:09:30.262361Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=10000) \n",
    "# fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=0000)\n",
    "# gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=50000)\n",
    "# rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=50000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data,], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Function to calculate reconstruction errors\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Step 2: Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Step 3: Plot the reconstruction error distribution to help set the threshold\n",
    "plt.hist(reconstruction_errors, bins=50)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Dynamically calculate the threshold based on the mean and standard deviation of reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "\n",
    "# Set threshold as mean + 2 standard deviations\n",
    "threshold = mean_error + 2 * std_error\n",
    "print(f\"Dynamically calculated threshold based on data: {threshold}\")\n",
    "\n",
    "# You can also choose to use the 95th percentile as an alternative:\n",
    "# threshold = np.percentile(reconstruction_errors, 95)\n",
    "# print(f\"Threshold based on 95th percentile: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies and get their corresponding row indices\n",
    "def detect_anomalies_with_indices(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # List to store indices of the anomalies\n",
    "    idx = 0  # Index counter for the data rows\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)  # Store the index of the anomaly\n",
    "        idx += 1  # Increment index counter\n",
    "    return anomalies, anomaly_indices\n",
    "\n",
    "# Detect anomalies and get their indices\n",
    "anomalies, anomaly_indices = detect_anomalies_with_indices(dataloader, model, threshold)\n",
    "\n",
    "# Print the number of anomalies detected and their corresponding row indices\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "print(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ef59d-0dca-42bf-96be-5357fc92f9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185ee2f-91e0-4d8c-aa75-398a659ad8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6541f71-d8bc-4bee-988b-5b6d9cadd2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e4e85-b886-48c3-8098-bda550b6970e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-18T13:09:30.280401Z",
     "iopub.status.idle": "2024-09-18T13:09:30.282099Z",
     "shell.execute_reply": "2024-09-18T13:09:30.282099Z",
     "shell.execute_reply.started": "2024-09-18T13:09:30.282099Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=10000) \n",
    "# fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=0000)\n",
    "# gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=50000)\n",
    "# rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=50000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Function to calculate reconstruction errors\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Step 2: Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Step 3: Dynamically calculate the threshold based on the mean and standard deviation of reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "\n",
    "# Set threshold as mean + 2 standard deviations\n",
    "threshold = mean_error + 2 * std_error\n",
    "print(f\"Dynamically calculated threshold based on data: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies and get their corresponding row indices\n",
    "def detect_anomalies_with_indices(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # List to store indices of the anomalies\n",
    "    idx = 0  # Index counter for the data rows\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)  # Store the index of the anomaly\n",
    "        idx += 1  # Increment index counter\n",
    "    return anomalies, anomaly_indices\n",
    "\n",
    "# Detect anomalies and get their indices\n",
    "anomalies, anomaly_indices = detect_anomalies_with_indices(dataloader, model, threshold)\n",
    "\n",
    "# Print the number of anomalies detected and their corresponding row indices\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "print(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Section\n",
    "# ==============================\n",
    "\n",
    "# A. Log Anomalies\n",
    "def log_anomalies(anomaly_indices, reconstruction_errors, data):\n",
    "    # Log anomalies with details\n",
    "    with open(\"anomaly_log.txt\", \"w\") as log_file:\n",
    "        for idx in anomaly_indices:\n",
    "            log_file.write(f\"Anomaly at row {idx}, Error: {reconstruction_errors[idx]}, Data: {data[idx]}\\n\")\n",
    "    print(\"Anomalies logged.\")\n",
    "\n",
    "log_anomalies(anomaly_indices, reconstruction_errors, data_tensor)\n",
    "\n",
    "# B. Alert Driver if critical anomaly is detected\n",
    "def alert_driver(message):\n",
    "    print(f\"Driver Alert: {message}\")\n",
    "\n",
    "# If critical anomaly is detected (e.g., anomaly in sensor data, ECU data), alert driver\n",
    "for idx in anomaly_indices:\n",
    "    alert_driver(f\"Critical anomaly detected at row {idx}\")\n",
    "\n",
    "# C. Isolate the ECU if compromised (pseudo-code)\n",
    "def isolate_ecu(ecu_id):\n",
    "    print(f\"ECU {ecu_id} is isolated due to suspicious activity.\")\n",
    "    # Implement code to disable or disconnect the ECU\n",
    "\n",
    "# Example: Isolate a specific ECU if anomaly is related to that ECU (replace 'ecu_id' with real data)\n",
    "isolate_ecu('ECU_1')\n",
    "\n",
    "# D. Trigger Safe Mode in case of critical failure\n",
    "def trigger_safe_mode():\n",
    "    print(\"Entering safe mode due to anomaly detection...\")\n",
    "\n",
    "# Trigger safe mode for severe anomalies\n",
    "if len(anomaly_indices) > 5:  # Example condition: more than 5 anomalies trigger safe mode\n",
    "    trigger_safe_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc762632-835c-4349-b51a-e502619602bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7813f-a378-4299-9096-18e8d19104c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b2e6f4-c11d-4d0f-85cc-9d436d6e3c36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T06:28:56.041027Z",
     "iopub.status.busy": "2024-09-19T06:28:56.041027Z",
     "iopub.status.idle": "2024-09-19T07:36:44.159705Z",
     "shell.execute_reply": "2024-09-19T07:36:44.153808Z",
     "shell.execute_reply.started": "2024-09-19T06:28:56.041027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 11:59:08,110 - INFO - Data loaded and concatenated. Total rows: 120000, Total columns: 32\n",
      "2024-09-19 11:59:21,522 - INFO - Data preprocessing complete. Shape after preprocessing: (120000, 5569)\n",
      "2024-09-19 11:59:22,725 - INFO - DataLoader created with batch size 64.\n",
      "2024-09-19 11:59:22,795 - INFO - Autoencoder initialized with input size 5569.\n",
      "2024-09-19 11:59:55,244 - INFO - Epoch 1/50, Loss: 0.002494103740900755\n",
      "2024-09-19 12:00:25,075 - INFO - Epoch 2/50, Loss: 0.0017179318238049746\n",
      "2024-09-19 12:01:01,693 - INFO - Epoch 3/50, Loss: 0.0016020906623452902\n",
      "2024-09-19 12:02:15,799 - INFO - Epoch 4/50, Loss: 0.001316692098043859\n",
      "2024-09-19 12:03:51,265 - INFO - Epoch 5/50, Loss: 0.0015014332020655274\n",
      "2024-09-19 12:05:15,804 - INFO - Epoch 6/50, Loss: 0.0018789664609357715\n",
      "2024-09-19 12:06:57,066 - INFO - Epoch 7/50, Loss: 0.0015660455683246255\n",
      "2024-09-19 12:08:22,567 - INFO - Epoch 8/50, Loss: 0.0016733399825170636\n",
      "2024-09-19 12:09:39,459 - INFO - Epoch 9/50, Loss: 0.0015997507143765688\n",
      "2024-09-19 12:11:14,639 - INFO - Epoch 10/50, Loss: 0.0015469666104763746\n",
      "2024-09-19 12:12:43,811 - INFO - Epoch 11/50, Loss: 0.0013320600846782327\n",
      "2024-09-19 12:14:02,817 - INFO - Epoch 12/50, Loss: 0.0014421023661270738\n",
      "2024-09-19 12:15:17,351 - INFO - Epoch 13/50, Loss: 0.0013236904051154852\n",
      "2024-09-19 12:16:46,854 - INFO - Epoch 14/50, Loss: 0.0014067367883399129\n",
      "2024-09-19 12:18:13,304 - INFO - Epoch 15/50, Loss: 0.001610300038009882\n",
      "2024-09-19 12:19:43,036 - INFO - Epoch 16/50, Loss: 0.0014646321069449186\n",
      "2024-09-19 12:21:16,956 - INFO - Epoch 17/50, Loss: 0.0016633076593279839\n",
      "2024-09-19 12:22:44,281 - INFO - Epoch 18/50, Loss: 0.001317597576417029\n",
      "2024-09-19 12:24:05,385 - INFO - Epoch 19/50, Loss: 0.001383704598993063\n",
      "2024-09-19 12:25:27,343 - INFO - Epoch 20/50, Loss: 0.0013467698590829968\n",
      "2024-09-19 12:27:00,832 - INFO - Epoch 21/50, Loss: 0.0015393150970339775\n",
      "2024-09-19 12:28:25,423 - INFO - Epoch 22/50, Loss: 0.0018818695098161697\n",
      "2024-09-19 12:29:50,647 - INFO - Epoch 23/50, Loss: 0.0014334727311506867\n",
      "2024-09-19 12:31:22,800 - INFO - Epoch 24/50, Loss: 0.0016986277187243104\n",
      "2024-09-19 12:32:50,150 - INFO - Epoch 25/50, Loss: 0.0016551414737477899\n",
      "2024-09-19 12:34:16,424 - INFO - Epoch 26/50, Loss: 0.0013072722358629107\n",
      "2024-09-19 12:35:42,189 - INFO - Epoch 27/50, Loss: 0.001544355065561831\n",
      "2024-09-19 12:37:16,665 - INFO - Epoch 28/50, Loss: 0.0016976058250293136\n",
      "2024-09-19 12:38:37,496 - INFO - Epoch 29/50, Loss: 0.0015320935053750873\n",
      "2024-09-19 12:39:58,313 - INFO - Epoch 30/50, Loss: 0.0016738490667194128\n",
      "2024-09-19 12:41:18,358 - INFO - Epoch 31/50, Loss: 0.0017343928338959813\n",
      "2024-09-19 12:42:37,183 - INFO - Epoch 32/50, Loss: 0.0015253007877618074\n",
      "2024-09-19 12:43:51,752 - INFO - Epoch 33/50, Loss: 0.0013230672338977456\n",
      "2024-09-19 12:45:19,336 - INFO - Epoch 34/50, Loss: 0.001851084060035646\n",
      "2024-09-19 12:46:40,211 - INFO - Epoch 35/50, Loss: 0.0012648836709558964\n",
      "2024-09-19 12:47:59,298 - INFO - Epoch 36/50, Loss: 0.001706363633275032\n",
      "2024-09-19 12:49:25,473 - INFO - Epoch 37/50, Loss: 0.0016769899521023035\n",
      "2024-09-19 12:50:44,147 - INFO - Epoch 38/50, Loss: 0.0017162151634693146\n",
      "2024-09-19 12:51:59,766 - INFO - Epoch 39/50, Loss: 0.001994506688788533\n",
      "2024-09-19 12:53:17,843 - INFO - Epoch 40/50, Loss: 0.0015717432834208012\n",
      "2024-09-19 12:54:33,357 - INFO - Epoch 41/50, Loss: 0.0012888568453490734\n",
      "2024-09-19 12:55:51,782 - INFO - Epoch 42/50, Loss: 0.0013267035828903317\n",
      "2024-09-19 12:57:11,978 - INFO - Epoch 43/50, Loss: 0.0013073391746729612\n",
      "2024-09-19 12:58:32,340 - INFO - Epoch 44/50, Loss: 0.0014168997295200825\n",
      "2024-09-19 12:59:52,601 - INFO - Epoch 45/50, Loss: 0.001757098245434463\n",
      "2024-09-19 13:01:14,613 - INFO - Epoch 46/50, Loss: 0.0014659258304163814\n",
      "2024-09-19 13:02:31,929 - INFO - Epoch 47/50, Loss: 0.0013614291092380881\n",
      "2024-09-19 13:03:48,407 - INFO - Epoch 48/50, Loss: 0.0017572952201589942\n",
      "2024-09-19 13:05:07,399 - INFO - Epoch 49/50, Loss: 0.0013633857015520334\n",
      "2024-09-19 13:06:22,271 - INFO - Epoch 50/50, Loss: 0.0018240307690575719\n",
      "2024-09-19 13:06:33,087 - INFO - Reconstruction errors calculated.\n",
      "2024-09-19 13:06:33,098 - INFO - Dynamically calculated threshold based on data: 0.002000020431397429\n",
      "2024-09-19 13:06:43,802 - INFO - Anomalies detected at rows (indices): [20, 25, 65, 72, 75, 83, 191, 192, 209, 221, 275, 277, 279, 293, 297, 315, 322, 329, 330, 337, 338, 353, 362, 376, 392, 394, 399, 426, 474, 476, 515, 547, 563, 564, 566, 629, 684, 704, 712, 714, 765, 825, 826, 890, 969, 989, 1073, 1075, 1105, 1133, 1194, 1204, 1271, 1297, 1305, 1325, 1350, 1365, 1378, 1382, 1399, 1415, 1432, 1450, 1594, 1618, 1631, 1686, 1698, 1735, 1775, 1784, 1806, 1812, 1860, 1868]\n",
      "2024-09-19 13:06:43,936 - INFO - Anomalies logged.\n",
      "2024-09-19 13:06:43,946 - WARNING - Driver Alert: Critical anomaly detected at row 20\n",
      "2024-09-19 13:06:43,947 - WARNING - Driver Alert: Critical anomaly detected at row 25\n",
      "2024-09-19 13:06:43,947 - WARNING - Driver Alert: Critical anomaly detected at row 65\n",
      "2024-09-19 13:06:43,954 - WARNING - Driver Alert: Critical anomaly detected at row 72\n",
      "2024-09-19 13:06:43,957 - WARNING - Driver Alert: Critical anomaly detected at row 75\n",
      "2024-09-19 13:06:43,957 - WARNING - Driver Alert: Critical anomaly detected at row 83\n",
      "2024-09-19 13:06:43,957 - WARNING - Driver Alert: Critical anomaly detected at row 191\n",
      "2024-09-19 13:06:43,963 - WARNING - Driver Alert: Critical anomaly detected at row 192\n",
      "2024-09-19 13:06:43,968 - WARNING - Driver Alert: Critical anomaly detected at row 209\n",
      "2024-09-19 13:06:43,970 - WARNING - Driver Alert: Critical anomaly detected at row 221\n",
      "2024-09-19 13:06:43,973 - WARNING - Driver Alert: Critical anomaly detected at row 275\n",
      "2024-09-19 13:06:43,975 - WARNING - Driver Alert: Critical anomaly detected at row 277\n",
      "2024-09-19 13:06:43,979 - WARNING - Driver Alert: Critical anomaly detected at row 279\n",
      "2024-09-19 13:06:43,979 - WARNING - Driver Alert: Critical anomaly detected at row 293\n",
      "2024-09-19 13:06:43,979 - WARNING - Driver Alert: Critical anomaly detected at row 297\n",
      "2024-09-19 13:06:43,979 - WARNING - Driver Alert: Critical anomaly detected at row 315\n",
      "2024-09-19 13:06:43,989 - WARNING - Driver Alert: Critical anomaly detected at row 322\n",
      "2024-09-19 13:06:43,991 - WARNING - Driver Alert: Critical anomaly detected at row 329\n",
      "2024-09-19 13:06:43,991 - WARNING - Driver Alert: Critical anomaly detected at row 330\n",
      "2024-09-19 13:06:43,996 - WARNING - Driver Alert: Critical anomaly detected at row 337\n",
      "2024-09-19 13:06:43,996 - WARNING - Driver Alert: Critical anomaly detected at row 338\n",
      "2024-09-19 13:06:44,001 - WARNING - Driver Alert: Critical anomaly detected at row 353\n",
      "2024-09-19 13:06:44,003 - WARNING - Driver Alert: Critical anomaly detected at row 362\n",
      "2024-09-19 13:06:44,003 - WARNING - Driver Alert: Critical anomaly detected at row 376\n",
      "2024-09-19 13:06:44,003 - WARNING - Driver Alert: Critical anomaly detected at row 392\n",
      "2024-09-19 13:06:44,003 - WARNING - Driver Alert: Critical anomaly detected at row 394\n",
      "2024-09-19 13:06:44,013 - WARNING - Driver Alert: Critical anomaly detected at row 399\n",
      "2024-09-19 13:06:44,013 - WARNING - Driver Alert: Critical anomaly detected at row 426\n",
      "2024-09-19 13:06:44,013 - WARNING - Driver Alert: Critical anomaly detected at row 474\n",
      "2024-09-19 13:06:44,013 - WARNING - Driver Alert: Critical anomaly detected at row 476\n",
      "2024-09-19 13:06:44,013 - WARNING - Driver Alert: Critical anomaly detected at row 515\n",
      "2024-09-19 13:06:44,023 - WARNING - Driver Alert: Critical anomaly detected at row 547\n",
      "2024-09-19 13:06:44,023 - WARNING - Driver Alert: Critical anomaly detected at row 563\n",
      "2024-09-19 13:06:44,027 - WARNING - Driver Alert: Critical anomaly detected at row 564\n",
      "2024-09-19 13:06:44,027 - WARNING - Driver Alert: Critical anomaly detected at row 566\n",
      "2024-09-19 13:06:44,034 - WARNING - Driver Alert: Critical anomaly detected at row 629\n",
      "2024-09-19 13:06:44,034 - WARNING - Driver Alert: Critical anomaly detected at row 684\n",
      "2024-09-19 13:06:44,040 - WARNING - Driver Alert: Critical anomaly detected at row 704\n",
      "2024-09-19 13:06:44,040 - WARNING - Driver Alert: Critical anomaly detected at row 712\n",
      "2024-09-19 13:06:44,044 - WARNING - Driver Alert: Critical anomaly detected at row 714\n",
      "2024-09-19 13:06:44,044 - WARNING - Driver Alert: Critical anomaly detected at row 765\n",
      "2024-09-19 13:06:44,044 - WARNING - Driver Alert: Critical anomaly detected at row 825\n",
      "2024-09-19 13:06:44,050 - WARNING - Driver Alert: Critical anomaly detected at row 826\n",
      "2024-09-19 13:06:44,050 - WARNING - Driver Alert: Critical anomaly detected at row 890\n",
      "2024-09-19 13:06:44,055 - WARNING - Driver Alert: Critical anomaly detected at row 969\n",
      "2024-09-19 13:06:44,059 - WARNING - Driver Alert: Critical anomaly detected at row 989\n",
      "2024-09-19 13:06:44,059 - WARNING - Driver Alert: Critical anomaly detected at row 1073\n",
      "2024-09-19 13:06:44,059 - WARNING - Driver Alert: Critical anomaly detected at row 1075\n",
      "2024-09-19 13:06:44,065 - WARNING - Driver Alert: Critical anomaly detected at row 1105\n",
      "2024-09-19 13:06:44,066 - WARNING - Driver Alert: Critical anomaly detected at row 1133\n",
      "2024-09-19 13:06:44,066 - WARNING - Driver Alert: Critical anomaly detected at row 1194\n",
      "2024-09-19 13:06:44,066 - WARNING - Driver Alert: Critical anomaly detected at row 1204\n",
      "2024-09-19 13:06:44,076 - WARNING - Driver Alert: Critical anomaly detected at row 1271\n",
      "2024-09-19 13:06:44,077 - WARNING - Driver Alert: Critical anomaly detected at row 1297\n",
      "2024-09-19 13:06:44,077 - WARNING - Driver Alert: Critical anomaly detected at row 1305\n",
      "2024-09-19 13:06:44,077 - WARNING - Driver Alert: Critical anomaly detected at row 1325\n",
      "2024-09-19 13:06:44,086 - WARNING - Driver Alert: Critical anomaly detected at row 1350\n",
      "2024-09-19 13:06:44,086 - WARNING - Driver Alert: Critical anomaly detected at row 1365\n",
      "2024-09-19 13:06:44,091 - WARNING - Driver Alert: Critical anomaly detected at row 1378\n",
      "2024-09-19 13:06:44,091 - WARNING - Driver Alert: Critical anomaly detected at row 1382\n",
      "2024-09-19 13:06:44,091 - WARNING - Driver Alert: Critical anomaly detected at row 1399\n",
      "2024-09-19 13:06:44,091 - WARNING - Driver Alert: Critical anomaly detected at row 1415\n",
      "2024-09-19 13:06:44,091 - WARNING - Driver Alert: Critical anomaly detected at row 1432\n",
      "2024-09-19 13:06:44,091 - WARNING - Driver Alert: Critical anomaly detected at row 1450\n",
      "2024-09-19 13:06:44,091 - WARNING - Driver Alert: Critical anomaly detected at row 1594\n",
      "2024-09-19 13:06:44,091 - WARNING - Driver Alert: Critical anomaly detected at row 1618\n",
      "2024-09-19 13:06:44,107 - WARNING - Driver Alert: Critical anomaly detected at row 1631\n",
      "2024-09-19 13:06:44,107 - WARNING - Driver Alert: Critical anomaly detected at row 1686\n",
      "2024-09-19 13:06:44,107 - WARNING - Driver Alert: Critical anomaly detected at row 1698\n",
      "2024-09-19 13:06:44,116 - WARNING - Driver Alert: Critical anomaly detected at row 1735\n",
      "2024-09-19 13:06:44,116 - WARNING - Driver Alert: Critical anomaly detected at row 1775\n",
      "2024-09-19 13:06:44,122 - WARNING - Driver Alert: Critical anomaly detected at row 1784\n",
      "2024-09-19 13:06:44,122 - WARNING - Driver Alert: Critical anomaly detected at row 1806\n",
      "2024-09-19 13:06:44,126 - WARNING - Driver Alert: Critical anomaly detected at row 1812\n",
      "2024-09-19 13:06:44,129 - WARNING - Driver Alert: Critical anomaly detected at row 1860\n",
      "2024-09-19 13:06:44,133 - WARNING - Driver Alert: Critical anomaly detected at row 1868\n",
      "2024-09-19 13:06:44,136 - ERROR - ECU ECU_1 is isolated due to suspicious activity.\n",
      "2024-09-19 13:06:44,140 - CRITICAL - Entering safe mode due to anomaly detection...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# ==============================\n",
    "# Logging Configuration\n",
    "# ==============================\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Define the format\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"system_log.log\"),  # Save logs to a file\n",
    "        logging.StreamHandler()  # Also display logs on the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=30000) \n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=30000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=30000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=30000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data], axis=0)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "logging.info(f\"Data loaded and concatenated. Total rows: {data.shape[0]}, Total columns: {data.shape[1]}\")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "logging.info(f\"Data preprocessing complete. Shape after preprocessing: {data_preprocessed.shape}\")\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "logging.info(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "logging.info(f\"Autoencoder initialized with input size {input_size}.\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Function to calculate reconstruction errors\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    logging.info(\"Reconstruction errors calculated.\")\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Step 2: Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Step 3: Dynamically calculate the threshold based on the mean and standard deviation of reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "\n",
    "# Set threshold as mean + 2 standard deviations\n",
    "threshold = mean_error + 2 * std_error\n",
    "logging.info(f\"Dynamically calculated threshold based on data: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies and get their corresponding row indices\n",
    "def detect_anomalies_with_indices(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # List to store indices of the anomalies\n",
    "    idx = 0  # Index counter for the data rows\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)  # Store the index of the anomaly\n",
    "        idx += 1  # Increment index counter\n",
    "    logging.info(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n",
    "    return anomalies, anomaly_indices\n",
    "\n",
    "# Detect anomalies and get their indices\n",
    "anomalies, anomaly_indices = detect_anomalies_with_indices(dataloader, model, threshold)\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Section with Logging\n",
    "# ==============================\n",
    "\n",
    "# A. Log Anomalies\n",
    "def log_anomalies(anomaly_indices, reconstruction_errors, data):\n",
    "    # Log anomalies with details\n",
    "    with open(\"anomaly_log.txt\", \"w\") as log_file:\n",
    "        for idx in anomaly_indices:\n",
    "            log_file.write(f\"Anomaly at row {idx}, Error: {reconstruction_errors[idx]}, Data: {data[idx]}\\n\")\n",
    "    logging.info(\"Anomalies logged.\")\n",
    "\n",
    "log_anomalies(anomaly_indices, reconstruction_errors, data_tensor)\n",
    "\n",
    "# B. Alert Driver if critical anomaly is detected\n",
    "def alert_driver(message):\n",
    "    logging.warning(f\"Driver Alert: {message}\")\n",
    "\n",
    "# If critical anomaly is detected (e.g., anomaly in sensor data, ECU data), alert driver\n",
    "for idx in anomaly_indices:\n",
    "    alert_driver(f\"Critical anomaly detected at row {idx}\")\n",
    "\n",
    "# C. Isolate the ECU if compromised (pseudo-code)\n",
    "def isolate_ecu(ecu_id):\n",
    "    logging.error(f\"ECU {ecu_id} is isolated due to suspicious activity.\")\n",
    "\n",
    "# Example: Isolate a specific ECU if anomaly is related to that ECU (replace 'ecu_id' with real data)\n",
    "isolate_ecu('ECU_1')\n",
    "\n",
    "# D. Trigger Safe Mode in case of critical failure\n",
    "def trigger_safe_mode():\n",
    "    logging.critical(\"Entering safe mode due to anomaly detection...\")\n",
    "\n",
    "# Trigger safe mode for severe anomalies\n",
    "if len(anomaly_indices) > 5:  # Example condition: more than 5 anomalies trigger safe mode\n",
    "    trigger_safe_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde4735-f1c7-40b8-9b94-ba8a2f0c88f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b813b-f651-45cb-b042-0d1640a0cb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87473726-5699-4daa-8ece-235464602753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a8a1c-4569-4d50-b4e0-5d7eaad95f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662bce8-593e-43c0-aa97-2a1e06ccbd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d1f76-5331-42ac-afa2-cdc595e75cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
