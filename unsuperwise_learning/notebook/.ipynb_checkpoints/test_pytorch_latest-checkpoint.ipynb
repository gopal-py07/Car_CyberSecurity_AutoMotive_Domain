{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94601bc-f99b-46a6-a39c-7e97c2ca2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=50000)\n",
    "# fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=50000)\n",
    "# gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=50000)\n",
    "# rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=50000)\n",
    "\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
