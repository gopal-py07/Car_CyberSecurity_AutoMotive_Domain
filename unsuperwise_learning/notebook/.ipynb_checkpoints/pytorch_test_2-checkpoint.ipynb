{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9704c522-b220-49cb-985e-f022eebaddb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T08:56:29.338765Z",
     "iopub.status.busy": "2024-09-18T08:56:29.338765Z",
     "iopub.status.idle": "2024-09-18T08:56:38.068169Z",
     "shell.execute_reply": "2024-09-18T08:56:38.068169Z",
     "shell.execute_reply.started": "2024-09-18T08:56:29.338765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 40000\n",
      "Number of batches in DataLoader: 625\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "from scipy import sparse\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing1.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting unsupervised learning data preprocessing.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=10000)\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=10000)\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=10000)\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=10000)\n",
    "    logging.info('Datasets loaded successfully with the first 1000 rows from each file.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataset = TensorDataset(X_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "logging.info(f'DataLoader created with batch size of 64. Total batches: {len(data_loader)}')\n",
    "\n",
    "# Print summary and log completion\n",
    "print(f\"Number of data points: {len(X_tensor)}\")\n",
    "print(f\"Number of batches in DataLoader: {len(data_loader)}\")\n",
    "logging.info('Data preprocessing completed successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d018f3e-1619-46f4-95e1-16f2d0c9052f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T08:56:38.068169Z",
     "iopub.status.busy": "2024-09-18T08:56:38.068169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: nan\n",
      "Epoch [2/50], Loss: nan\n",
      "Epoch [3/50], Loss: nan\n",
      "Epoch [4/50], Loss: nan\n",
      "Epoch [5/50], Loss: nan\n",
      "Epoch [6/50], Loss: nan\n",
      "Epoch [7/50], Loss: nan\n",
      "Epoch [8/50], Loss: nan\n",
      "Epoch [9/50], Loss: nan\n",
      "Epoch [10/50], Loss: nan\n",
      "Epoch [11/50], Loss: nan\n",
      "Epoch [12/50], Loss: nan\n",
      "Epoch [13/50], Loss: nan\n",
      "Epoch [14/50], Loss: nan\n",
      "Epoch [15/50], Loss: nan\n",
      "Epoch [16/50], Loss: nan\n",
      "Epoch [17/50], Loss: nan\n",
      "Epoch [18/50], Loss: nan\n",
      "Epoch [19/50], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing_anomaly_detection.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting data preprocessing for anomaly detection.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=20000)\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=20000)\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=20000)\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=20000)\n",
    "    logging.info('Datasets loaded successfully with the first 1000 rows from each file.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "logging.info('Numeric data normalized using StandardScaler.')\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "logging.info('Data concatenated and converted to dense format for PyTorch.')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Step 1: Train-Test Split\n",
    "X_train, X_test = train_test_split(X_tensor, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Data split into training and test sets with a test size of 20%.\")\n",
    "\n",
    "# Create DataLoaders for both training and test sets\n",
    "train_dataset = TensorDataset(X_train)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "logging.info('DataLoader created for both training and test sets.')\n",
    "\n",
    "# Step 2: Define the Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid to ensure outputs are between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "input_dim = X_tensor.shape[1]  # Number of input features\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "logging.info(f\"Autoencoder model initialized with input dimension: {input_dim}.\")\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "logging.info('Loss function and optimizer set up.')\n",
    "\n",
    "# Train the Autoencoder\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Compare output with input\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    logging.info(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "logging.info('Training complete.')\n",
    "\n",
    "# Step 4: Anomaly Detection\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    with torch.no_grad():  # No need to compute gradients in evaluation\n",
    "        for data in data_loader:\n",
    "            inputs = data[0]\n",
    "            outputs = model(inputs)\n",
    "            reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "            # If reconstruction error is above the threshold, flag as anomaly\n",
    "            anomalies += (reconstruction_error > threshold).cpu().numpy().tolist()\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = 0.02\n",
    "logging.info(f\"Anomaly detection threshold set to {threshold}.\")\n",
    "\n",
    "# Detect anomalies in the test set\n",
    "anomalies = detect_anomalies(test_loader, autoencoder, threshold)\n",
    "\n",
    "# Print the number of detected anomalies\n",
    "num_anomalies = sum(anomalies)\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "logging.info(f\"Number of anomalies detected: {num_anomalies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738be20a-4f93-4ef3-9f07-97adda4095de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing_anomaly_detection.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting data preprocessing for anomaly detection.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=1000)\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=1000)\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=1000)\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=1000)\n",
    "    logging.info('Datasets loaded successfully with the first 1000 rows from each file.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "logging.info('Numeric data normalized using StandardScaler.')\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "logging.info('Data concatenated and converted to dense format for PyTorch.')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Step 1: Train-Test Split\n",
    "X_train, X_test = train_test_split(X_tensor, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Data split into training and test sets with a test size of 20%.\")\n",
    "\n",
    "# Create DataLoaders for both training and test sets\n",
    "train_dataset = TensorDataset(X_train)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "logging.info('DataLoader created for both training and test sets.')\n",
    "\n",
    "# Step 2: Define the Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid to ensure outputs are between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "input_dim = X_tensor.shape[1]  # Number of input features\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "logging.info(f\"Autoencoder model initialized with input dimension: {input_dim}.\")\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "logging.info('Loss function and optimizer set up.')\n",
    "\n",
    "# Train the Autoencoder\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Compare output with input\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    logging.info(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "logging.info('Training complete.')\n",
    "\n",
    "# Step 4: Anomaly Detection\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    with torch.no_grad():  # No need to compute gradients in evaluation\n",
    "        for data in data_loader:\n",
    "            inputs = data[0]\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute reconstruction error (MSE between inputs and outputs)\n",
    "            reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "            \n",
    "            # Flag as anomaly if reconstruction error exceeds the threshold\n",
    "            anomalies += (reconstruction_error > threshold).cpu().numpy().tolist()\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = 0.02  # Adjust this value based on your data and testing\n",
    "logging.info(f\"Anomaly detection threshold set to {threshold}.\")\n",
    "\n",
    "# Detect anomalies in the test set\n",
    "anomalies = detect_anomalies(test_loader, autoencoder, threshold)\n",
    "num_anomalies = sum(anomalies)\n",
    "print(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "logging.info(f\"Number of anomalies detected: {num_anomalies}\")\n",
    "\n",
    "# Plot reconstruction errors (optional for better visualization)\n",
    "def plot_reconstruction_errors(data_loader, model):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs = data[0]\n",
    "            outputs = model(inputs)\n",
    "            reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "            reconstruction_errors += reconstruction_error.cpu().numpy().tolist()\n",
    "    \n",
    "    # Plot histogram of reconstruction errors\n",
    "    plt.hist(reconstruction_errors, bins=50)\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Number of Data Points')\n",
    "    plt.title('Reconstruction Errors of Test Data')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot errors\n",
    "plot_reconstruction_errors(test_loader, autoencoder)\n",
    "\n",
    "# Tuning the threshold based on the results\n",
    "threshold = 0.01  # Adjust this if you're missing anomalies or detecting too many\n",
    "anomalies = detect_anomalies(test_loader, autoencoder, threshold)\n",
    "print(f\"Number of anomalies detected after tuning: {sum(anomalies)}\")\n",
    "logging.info(f\"Number of anomalies detected after tuning: {sum(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291efb7-a991-41bb-8c3f-4fd3998379c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to set up logging\n",
    "def setup_logging(debug=False):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    file_handler = logging.FileHandler('../logs/data_preprocessing_anomaly_detection.log')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    if debug:\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "    else:\n",
    "        logger.info(\"Console logging is disabled; only logging to file.\")\n",
    "\n",
    "setup_logging(debug=False)  # Set to True to enable console logging, False for file only\n",
    "\n",
    "logging.info('Starting data preprocessing for anomaly detection.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=51000)\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=51000)\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=51000)\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=51000)\n",
    "    logging.info('Datasets loaded successfully with the first 1000 rows from each file.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "logging.info('Numeric data normalized using StandardScaler.')\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "logging.info('Data concatenated and converted to dense format for PyTorch.')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Step 1: Train-Test Split\n",
    "X_train, X_test = train_test_split(X_tensor, test_size=0.2, random_state=42)\n",
    "logging.info(f\"Data split into training and test sets with a test size of 20%.\")\n",
    "\n",
    "# Create DataLoaders for both training and test sets\n",
    "train_dataset = TensorDataset(X_train)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "logging.info('DataLoader created for both training and test sets.')\n",
    "\n",
    "# Step 2: Define the Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()  # Sigmoid to ensure outputs are between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "input_dim = X_tensor.shape[1]  # Number of input features\n",
    "autoencoder = Autoencoder(input_dim)\n",
    "logging.info(f\"Autoencoder model initialized with input dimension: {input_dim}.\")\n",
    "\n",
    "# Step 3: Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction loss\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "logging.info('Loss function and optimizer set up.')\n",
    "\n",
    "# Train the Autoencoder\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Compare output with input\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    logging.info(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "logging.info('Training complete.')\n",
    "\n",
    "# Step 4: Anomaly Detection\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()\n",
    "    anomalies = []\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs = data[0]\n",
    "            outputs = model(inputs)\n",
    "            reconstruction_error = torch.mean((outputs - inputs) ** 2, dim=1)\n",
    "            anomalies += (reconstruction_error > threshold).cpu().numpy().tolist()\n",
    "            reconstruction_errors += reconstruction_error.cpu().numpy().tolist()\n",
    "    \n",
    "    return anomalies, reconstruction_errors\n",
    "\n",
    "# Updated function to handle NaN values in reconstruction errors\n",
    "def plot_reconstruction_errors(data_loader, model):\n",
    "    model.eval()\n",
    "    _, reconstruction_errors = detect_anomalies(data_loader, model, threshold=0.02)\n",
    "    reconstruction_errors = np.array(reconstruction_errors)\n",
    "    \n",
    "    # Remove NaN values before plotting\n",
    "    clean_reconstruction_errors = reconstruction_errors[~np.isnan(reconstruction_errors)]\n",
    "    \n",
    "    if len(clean_reconstruction_errors) == 0:\n",
    "        print(\"No valid reconstruction errors to plot.\")\n",
    "        logging.warning(\"All reconstruction errors were NaN.\")\n",
    "        return\n",
    "    \n",
    "    # Plot histogram of reconstruction errors\n",
    "    plt.hist(clean_reconstruction_errors, bins=50)\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Number of Data Points')\n",
    "    plt.title('Reconstruction Errors of Test Data')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot errors after handling NaN values\n",
    "plot_reconstruction_errors(test_loader, autoencoder)\n",
    "\n",
    "# Tuning the threshold based on the results\n",
    "threshold = 0.01  # Adjust this if you're missing anomalies or detecting too many\n",
    "anomalies = detect_anomalies(test_loader, autoencoder, threshold)[0]\n",
    "print(f\"Number of anomalies detected after tuning: {sum(anomalies)}\")\n",
    "logging.info(f\"Number of anomalies detected after tuning: {sum(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba52a78-7c88-4e70-9ea9-beef27f95b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=51000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=51000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=51000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=51000)\n",
    "print(\"data loade\")\n",
    "\n",
    "# Concatenate datasets for training\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "\n",
    "# Data preprocessing: Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)  # Normalize the data\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data_tensor = torch.tensor(data_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_scaled.shape[1]  # Number of features (columns)\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf495fc8-2272-405e-9998-835d010a450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=51000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=51000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=51000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=51000)\n",
    "print(\"data loade\")\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed.toarray(), dtype=torch.float32) if isinstance(data_preprocessed, pd.DataFrame) else torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06337b1-dd03-418b-9371-a19993fb6aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef08ed5-f6b7-4db3-951b-ef8dcf96537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=20000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=20000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=20000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=20000)\n",
    "print(\"data loade\")\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908915cb-551d-45c4-9a1c-f17bb08565c9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-18T09:12:03.232297Z",
     "iopub.status.idle": "2024-09-18T09:12:03.232297Z",
     "shell.execute_reply": "2024-09-18T09:12:03.232297Z",
     "shell.execute_reply.started": "2024-09-18T09:12:03.232297Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=1000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=1000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=1000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=1000)\n",
    "print(\"data loade\")\n",
    "1\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7538cd-c9f0-4946-8332-218cf7b4a9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee1fcd-f80b-4abd-9e5e-1bc85deb8ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a782aeb-4417-4cb9-8a04-69a24f64d1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbaa977-b08b-40b9-b5e4-dcdbd1eef044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3624638-16d1-495f-ae95-4d296287cf59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T09:19:37.806932Z",
     "iopub.status.busy": "2024-09-18T09:19:37.805932Z",
     "iopub.status.idle": "2024-09-18T10:18:00.309755Z",
     "shell.execute_reply": "2024-09-18T10:18:00.305185Z",
     "shell.execute_reply.started": "2024-09-18T09:19:37.806932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data Shape: (100000, 4658)\n",
      "Epoch 1/50, Loss: 0.0016622989205643535\n",
      "Epoch 2/50, Loss: 0.0019247063901275396\n",
      "Epoch 3/50, Loss: 0.001487645786255598\n",
      "Epoch 4/50, Loss: 0.002083810279145837\n",
      "Epoch 5/50, Loss: 0.002271123230457306\n",
      "Epoch 6/50, Loss: 0.0020341912750154734\n",
      "Epoch 7/50, Loss: 0.001570958993397653\n",
      "Epoch 8/50, Loss: 0.001594410976395011\n",
      "Epoch 9/50, Loss: 0.0014808414271101356\n",
      "Epoch 10/50, Loss: 0.0019462417112663388\n",
      "Epoch 11/50, Loss: 0.002115114824846387\n",
      "Epoch 12/50, Loss: 0.0019734478555619717\n",
      "Epoch 13/50, Loss: 0.001399946166202426\n",
      "Epoch 14/50, Loss: 0.0020991209894418716\n",
      "Epoch 15/50, Loss: 0.0013477153843268752\n",
      "Epoch 16/50, Loss: 0.0015800240216776729\n",
      "Epoch 17/50, Loss: 0.00208490202203393\n",
      "Epoch 18/50, Loss: 0.001667408854700625\n",
      "Epoch 19/50, Loss: 0.0016002601478248835\n",
      "Epoch 20/50, Loss: 0.0013892956776544452\n",
      "Epoch 21/50, Loss: 0.001449048169888556\n",
      "Epoch 22/50, Loss: 0.0013857238227501512\n",
      "Epoch 23/50, Loss: 0.001347490237094462\n",
      "Epoch 24/50, Loss: 0.0026340712793171406\n",
      "Epoch 25/50, Loss: 0.0015233804006129503\n",
      "Epoch 26/50, Loss: 0.0015637304168194532\n",
      "Epoch 27/50, Loss: 0.0018334458582103252\n",
      "Epoch 28/50, Loss: 0.0013166783610358834\n",
      "Epoch 29/50, Loss: 0.0014587299665436149\n",
      "Epoch 30/50, Loss: 0.0014781916979700327\n",
      "Epoch 31/50, Loss: 0.00163696997333318\n",
      "Epoch 32/50, Loss: 0.0013384545454755425\n",
      "Epoch 33/50, Loss: 0.001483786734752357\n",
      "Epoch 34/50, Loss: 0.0014453927287831903\n",
      "Epoch 35/50, Loss: 0.0014834640314802527\n",
      "Epoch 36/50, Loss: 0.0014112244825810194\n",
      "Epoch 37/50, Loss: 0.001306184334680438\n",
      "Epoch 38/50, Loss: 0.0018240128410980105\n",
      "Epoch 39/50, Loss: 0.001973213627934456\n",
      "Epoch 40/50, Loss: 0.001983215333893895\n",
      "Epoch 41/50, Loss: 0.0011907804291695356\n",
      "Epoch 42/50, Loss: 0.0015629241243004799\n",
      "Epoch 43/50, Loss: 0.001411588629707694\n",
      "Epoch 44/50, Loss: 0.0011122201103717089\n",
      "Epoch 45/50, Loss: 0.0018185697263106704\n",
      "Epoch 46/50, Loss: 0.0015844415174797177\n",
      "Epoch 47/50, Loss: 0.0011479833628982306\n",
      "Epoch 48/50, Loss: 0.0012482141610234976\n",
      "Epoch 49/50, Loss: 0.0016801574965938926\n",
      "Epoch 50/50, Loss: 0.0018233751179650426\n",
      "Number of anomalies detected: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=50000)\n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), nrows=50000)\n",
    "# gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=50000)\n",
    "# rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=50000)\n",
    "\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Anomaly Detection: Detect anomalies based on reconstruction error\n",
    "def detect_anomalies(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "    return anomalies\n",
    "\n",
    "# Set a threshold based on training loss (you can adjust this threshold)\n",
    "threshold = 0.02  # Example threshold, fine-tune based on your data\n",
    "anomalies = detect_anomalies(dataloader, model, threshold)\n",
    "\n",
    "# Display the number of anomalies detected\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "110cecd8-5a2c-4cb4-b7f5-5ed98c32730b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T10:40:59.839377Z",
     "iopub.status.busy": "2024-09-18T10:40:59.839377Z",
     "iopub.status.idle": "2024-09-18T10:42:49.769240Z",
     "shell.execute_reply": "2024-09-18T10:42:49.769240Z",
     "shell.execute_reply.started": "2024-09-18T10:40:59.839377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data Shape: (10000, 634)\n",
      "Epoch 1/50, Loss: 0.04461054131388664\n",
      "Epoch 2/50, Loss: 0.02323850989341736\n",
      "Epoch 3/50, Loss: 0.0038485180120915174\n",
      "Epoch 4/50, Loss: 0.004604123532772064\n",
      "Epoch 5/50, Loss: 0.0063688792288303375\n",
      "Epoch 6/50, Loss: 0.006351419258862734\n",
      "Epoch 7/50, Loss: 0.006595652084797621\n",
      "Epoch 8/50, Loss: 0.004780030809342861\n",
      "Epoch 9/50, Loss: 0.0038774702697992325\n",
      "Epoch 10/50, Loss: 0.02172872982919216\n",
      "Epoch 11/50, Loss: 0.00397769920527935\n",
      "Epoch 12/50, Loss: 0.006448171101510525\n",
      "Epoch 13/50, Loss: 0.0035933763720095158\n",
      "Epoch 14/50, Loss: 0.0056982445530593395\n",
      "Epoch 15/50, Loss: 0.023714374750852585\n",
      "Epoch 16/50, Loss: 0.004349907394498587\n",
      "Epoch 17/50, Loss: 0.004434955306351185\n",
      "Epoch 18/50, Loss: 0.00474075973033905\n",
      "Epoch 19/50, Loss: 0.0035572992637753487\n",
      "Epoch 20/50, Loss: 0.004829978570342064\n",
      "Epoch 21/50, Loss: 0.006314925849437714\n",
      "Epoch 22/50, Loss: 0.006014320068061352\n",
      "Epoch 23/50, Loss: 0.0036594788543879986\n",
      "Epoch 24/50, Loss: 0.00465913163498044\n",
      "Epoch 25/50, Loss: 0.021393906325101852\n",
      "Epoch 26/50, Loss: 0.0042907800525426865\n",
      "Epoch 27/50, Loss: 0.004115737043321133\n",
      "Epoch 28/50, Loss: 0.005787995178252459\n",
      "Epoch 29/50, Loss: 0.0039463709108531475\n",
      "Epoch 30/50, Loss: 0.004545042756944895\n",
      "Epoch 31/50, Loss: 0.00621171435341239\n",
      "Epoch 32/50, Loss: 0.0032644339371472597\n",
      "Epoch 33/50, Loss: 0.0053125121630728245\n",
      "Epoch 34/50, Loss: 0.006579951383173466\n",
      "Epoch 35/50, Loss: 0.005132880061864853\n",
      "Epoch 36/50, Loss: 0.021788006648421288\n",
      "Epoch 37/50, Loss: 0.006726569030433893\n",
      "Epoch 38/50, Loss: 0.0034048075322061777\n",
      "Epoch 39/50, Loss: 0.007309709209948778\n",
      "Epoch 40/50, Loss: 0.005207711830735207\n",
      "Epoch 41/50, Loss: 0.0047677489928901196\n",
      "Epoch 42/50, Loss: 0.00676126079633832\n",
      "Epoch 43/50, Loss: 0.004399571567773819\n",
      "Epoch 44/50, Loss: 0.003099351190030575\n",
      "Epoch 45/50, Loss: 0.0038803175557404757\n",
      "Epoch 46/50, Loss: 0.02325967699289322\n",
      "Epoch 47/50, Loss: 0.005730882287025452\n",
      "Epoch 48/50, Loss: 0.0031734355725347996\n",
      "Epoch 49/50, Loss: 0.006484804209321737\n",
      "Epoch 50/50, Loss: 0.005669009871780872\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamically calculated threshold based on data: 0.01243576926917553\n",
      "Number of anomalies detected: 11\n",
      "Anomalies detected at rows (indices): [31, 39, 42, 51, 57, 58, 78, 80, 107, 113, 140]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=10000) \n",
    "# fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=0000)\n",
    "# gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=50000)\n",
    "# rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=50000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data,], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Function to calculate reconstruction errors\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Step 2: Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Step 3: Plot the reconstruction error distribution to help set the threshold\n",
    "plt.hist(reconstruction_errors, bins=50)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Dynamically calculate the threshold based on the mean and standard deviation of reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "\n",
    "# Set threshold as mean + 2 standard deviations\n",
    "threshold = mean_error + 2 * std_error\n",
    "print(f\"Dynamically calculated threshold based on data: {threshold}\")\n",
    "\n",
    "# You can also choose to use the 95th percentile as an alternative:\n",
    "# threshold = np.percentile(reconstruction_errors, 95)\n",
    "# print(f\"Threshold based on 95th percentile: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies and get their corresponding row indices\n",
    "def detect_anomalies_with_indices(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # List to store indices of the anomalies\n",
    "    idx = 0  # Index counter for the data rows\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)  # Store the index of the anomaly\n",
    "        idx += 1  # Increment index counter\n",
    "    return anomalies, anomaly_indices\n",
    "\n",
    "# Detect anomalies and get their indices\n",
    "anomalies, anomaly_indices = detect_anomalies_with_indices(dataloader, model, threshold)\n",
    "\n",
    "# Print the number of anomalies detected and their corresponding row indices\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "print(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ef59d-0dca-42bf-96be-5357fc92f9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185ee2f-91e0-4d8c-aa75-398a659ad8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6541f71-d8bc-4bee-988b-5b6d9cadd2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "594e4e85-b886-48c3-8098-bda550b6970e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T10:50:21.111556Z",
     "iopub.status.busy": "2024-09-18T10:50:21.105481Z",
     "iopub.status.idle": "2024-09-18T10:52:19.994259Z",
     "shell.execute_reply": "2024-09-18T10:52:19.994259Z",
     "shell.execute_reply.started": "2024-09-18T10:50:21.111556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data Shape: (10000, 634)\n",
      "Epoch 1/50, Loss: 0.011316251009702682\n",
      "Epoch 2/50, Loss: 0.004800365772098303\n",
      "Epoch 3/50, Loss: 0.025145819410681725\n",
      "Epoch 4/50, Loss: 0.007702808827161789\n",
      "Epoch 5/50, Loss: 0.00718501303344965\n",
      "Epoch 6/50, Loss: 0.025675611570477486\n",
      "Epoch 7/50, Loss: 0.007322270888835192\n",
      "Epoch 8/50, Loss: 0.023511983454227448\n",
      "Epoch 9/50, Loss: 0.004434223286807537\n",
      "Epoch 10/50, Loss: 0.006126193795353174\n",
      "Epoch 11/50, Loss: 0.00426196213811636\n",
      "Epoch 12/50, Loss: 0.0045079258270561695\n",
      "Epoch 13/50, Loss: 0.0053510903380811214\n",
      "Epoch 14/50, Loss: 0.005039469804614782\n",
      "Epoch 15/50, Loss: 0.004986872896552086\n",
      "Epoch 16/50, Loss: 0.00600180821493268\n",
      "Epoch 17/50, Loss: 0.0034828162752091885\n",
      "Epoch 18/50, Loss: 0.006794850341975689\n",
      "Epoch 19/50, Loss: 0.005401878617703915\n",
      "Epoch 20/50, Loss: 0.005422487389296293\n",
      "Epoch 21/50, Loss: 0.006432144436985254\n",
      "Epoch 22/50, Loss: 0.02249911054968834\n",
      "Epoch 23/50, Loss: 0.004917270038276911\n",
      "Epoch 24/50, Loss: 0.0060349698178470135\n",
      "Epoch 25/50, Loss: 0.006142890080809593\n",
      "Epoch 26/50, Loss: 0.0033190487883985043\n",
      "Epoch 27/50, Loss: 0.0026375139132142067\n",
      "Epoch 28/50, Loss: 0.00632573151960969\n",
      "Epoch 29/50, Loss: 0.005373928230255842\n",
      "Epoch 30/50, Loss: 0.005143137648701668\n",
      "Epoch 31/50, Loss: 0.005708225071430206\n",
      "Epoch 32/50, Loss: 0.002950015477836132\n",
      "Epoch 33/50, Loss: 0.0034246945288032293\n",
      "Epoch 34/50, Loss: 0.006218330468982458\n",
      "Epoch 35/50, Loss: 0.004272970836609602\n",
      "Epoch 36/50, Loss: 0.0053822691552340984\n",
      "Epoch 37/50, Loss: 0.005027953069657087\n",
      "Epoch 38/50, Loss: 0.005446659866720438\n",
      "Epoch 39/50, Loss: 0.006766697391867638\n",
      "Epoch 40/50, Loss: 0.006176893599331379\n",
      "Epoch 41/50, Loss: 0.02100280113518238\n",
      "Epoch 42/50, Loss: 0.006371473427861929\n",
      "Epoch 43/50, Loss: 0.006140390411019325\n",
      "Epoch 44/50, Loss: 0.005134129896759987\n",
      "Epoch 45/50, Loss: 0.0037910370156168938\n",
      "Epoch 46/50, Loss: 0.006763707846403122\n",
      "Epoch 47/50, Loss: 0.006860067602247\n",
      "Epoch 48/50, Loss: 0.004797324072569609\n",
      "Epoch 49/50, Loss: 0.004437873139977455\n",
      "Epoch 50/50, Loss: 0.005843904335051775\n",
      "Dynamically calculated threshold based on data: 0.012075576016693325\n",
      "Number of anomalies detected: 9\n",
      "Anomalies detected at rows (indices): [10, 55, 80, 88, 101, 114, 134, 136, 139]\n",
      "Anomalies logged.\n",
      "Driver Alert: Critical anomaly detected at row 10\n",
      "Driver Alert: Critical anomaly detected at row 55\n",
      "Driver Alert: Critical anomaly detected at row 80\n",
      "Driver Alert: Critical anomaly detected at row 88\n",
      "Driver Alert: Critical anomaly detected at row 101\n",
      "Driver Alert: Critical anomaly detected at row 114\n",
      "Driver Alert: Critical anomaly detected at row 134\n",
      "Driver Alert: Critical anomaly detected at row 136\n",
      "Driver Alert: Critical anomaly detected at row 139\n",
      "ECU ECU_1 is isolated due to suspicious activity.\n",
      "Entering safe mode due to anomaly detection...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=10000) \n",
    "# fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=0000)\n",
    "# gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=50000)\n",
    "# rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=50000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data], axis=0)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Preprocessed Data Shape: {data_preprocessed.shape}\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Function to calculate reconstruction errors\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Step 2: Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Step 3: Dynamically calculate the threshold based on the mean and standard deviation of reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "\n",
    "# Set threshold as mean + 2 standard deviations\n",
    "threshold = mean_error + 2 * std_error\n",
    "print(f\"Dynamically calculated threshold based on data: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies and get their corresponding row indices\n",
    "def detect_anomalies_with_indices(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # List to store indices of the anomalies\n",
    "    idx = 0  # Index counter for the data rows\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)  # Store the index of the anomaly\n",
    "        idx += 1  # Increment index counter\n",
    "    return anomalies, anomaly_indices\n",
    "\n",
    "# Detect anomalies and get their indices\n",
    "anomalies, anomaly_indices = detect_anomalies_with_indices(dataloader, model, threshold)\n",
    "\n",
    "# Print the number of anomalies detected and their corresponding row indices\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "print(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Section\n",
    "# ==============================\n",
    "\n",
    "# A. Log Anomalies\n",
    "def log_anomalies(anomaly_indices, reconstruction_errors, data):\n",
    "    # Log anomalies with details\n",
    "    with open(\"anomaly_log.txt\", \"w\") as log_file:\n",
    "        for idx in anomaly_indices:\n",
    "            log_file.write(f\"Anomaly at row {idx}, Error: {reconstruction_errors[idx]}, Data: {data[idx]}\\n\")\n",
    "    print(\"Anomalies logged.\")\n",
    "\n",
    "log_anomalies(anomaly_indices, reconstruction_errors, data_tensor)\n",
    "\n",
    "# B. Alert Driver if critical anomaly is detected\n",
    "def alert_driver(message):\n",
    "    print(f\"Driver Alert: {message}\")\n",
    "\n",
    "# If critical anomaly is detected (e.g., anomaly in sensor data, ECU data), alert driver\n",
    "for idx in anomaly_indices:\n",
    "    alert_driver(f\"Critical anomaly detected at row {idx}\")\n",
    "\n",
    "# C. Isolate the ECU if compromised (pseudo-code)\n",
    "def isolate_ecu(ecu_id):\n",
    "    print(f\"ECU {ecu_id} is isolated due to suspicious activity.\")\n",
    "    # Implement code to disable or disconnect the ECU\n",
    "\n",
    "# Example: Isolate a specific ECU if anomaly is related to that ECU (replace 'ecu_id' with real data)\n",
    "isolate_ecu('ECU_1')\n",
    "\n",
    "# D. Trigger Safe Mode in case of critical failure\n",
    "def trigger_safe_mode():\n",
    "    print(\"Entering safe mode due to anomaly detection...\")\n",
    "\n",
    "# Trigger safe mode for severe anomalies\n",
    "if len(anomaly_indices) > 5:  # Example condition: more than 5 anomalies trigger safe mode\n",
    "    trigger_safe_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc762632-835c-4349-b51a-e502619602bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7813f-a378-4299-9096-18e8d19104c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2e6f4-c11d-4d0f-85cc-9d436d6e3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# ==============================\n",
    "# Logging Configuration\n",
    "# ==============================\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Define the format\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"system_log.log\"),  # Save logs to a file\n",
    "        logging.StreamHandler()  # Also display logs on the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=30000) \n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=30000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=30000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=30000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data], axis=0)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "logging.info(f\"Data loaded and concatenated. Total rows: {data.shape[0]}, Total columns: {data.shape[1]}\")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "logging.info(f\"Data preprocessing complete. Shape after preprocessing: {data_preprocessed.shape}\")\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "logging.info(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "logging.info(f\"Autoencoder initialized with input size {input_size}.\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Function to calculate reconstruction errors\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    logging.info(\"Reconstruction errors calculated.\")\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Step 2: Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Step 3: Dynamically calculate the threshold based on the mean and standard deviation of reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "\n",
    "# Set threshold as mean + 2 standard deviations\n",
    "threshold = mean_error + 2 * std_error\n",
    "logging.info(f\"Dynamically calculated threshold based on data: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies and get their corresponding row indices\n",
    "def detect_anomalies_with_indices(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # List to store indices of the anomalies\n",
    "    idx = 0  # Index counter for the data rows\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)  # Store the index of the anomaly\n",
    "        idx += 1  # Increment index counter\n",
    "    logging.info(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n",
    "    return anomalies, anomaly_indices\n",
    "\n",
    "# Detect anomalies and get their indices\n",
    "anomalies, anomaly_indices = detect_anomalies_with_indices(dataloader, model, threshold)\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Section with Logging\n",
    "# ==============================\n",
    "\n",
    "# A. Log Anomalies\n",
    "def log_anomalies(anomaly_indices, reconstruction_errors, data):\n",
    "    # Log anomalies with details\n",
    "    with open(\"anomaly_log.txt\", \"w\") as log_file:\n",
    "        for idx in anomaly_indices:\n",
    "            log_file.write(f\"Anomaly at row {idx}, Error: {reconstruction_errors[idx]}, Data: {data[idx]}\\n\")\n",
    "    logging.info(\"Anomalies logged.\")\n",
    "\n",
    "log_anomalies(anomaly_indices, reconstruction_errors, data_tensor)\n",
    "\n",
    "# B. Alert Driver if critical anomaly is detected\n",
    "def alert_driver(message):\n",
    "    logging.warning(f\"Driver Alert: {message}\")\n",
    "\n",
    "# If critical anomaly is detected (e.g., anomaly in sensor data, ECU data), alert driver\n",
    "for idx in anomaly_indices:\n",
    "    alert_driver(f\"Critical anomaly detected at row {idx}\")\n",
    "\n",
    "# C. Isolate the ECU if compromised (pseudo-code)\n",
    "def isolate_ecu(ecu_id):\n",
    "    logging.error(f\"ECU {ecu_id} is isolated due to suspicious activity.\")\n",
    "\n",
    "# Example: Isolate a specific ECU if anomaly is related to that ECU (replace 'ecu_id' with real data)\n",
    "isolate_ecu('ECU_1')\n",
    "\n",
    "# D. Trigger Safe Mode in case of critical failure\n",
    "def trigger_safe_mode():\n",
    "    logging.critical(\"Entering safe mode due to anomaly detection...\")\n",
    "\n",
    "# Trigger safe mode for severe anomalies\n",
    "if len(anomaly_indices) > 5:  # Example condition: more than 5 anomalies trigger safe mode\n",
    "    trigger_safe_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c6fc3-5b5b-41e3-ab50-fedb2e70bcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
