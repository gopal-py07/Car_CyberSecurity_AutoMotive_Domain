{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722aa357-3dd2-4b37-aa38-b4e1a64cd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# ==============================\n",
    "# Logging Configuration\n",
    "# ==============================\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Define the format\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"system_log.log\"),  # Save logs to a file\n",
    "        logging.StreamHandler()  # Also display logs on the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the datasets\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load only the first 1000 rows of each dataset\n",
    "\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=3000) \n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=3000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=3000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=3000)\n",
    "\n",
    "# Concatenate datasets\n",
    "#data = pd.concat([dos_data], axis=0)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "logging.info(f\"Data loaded and concatenated. Total rows: {data.shape[0]}, Total columns: {data.shape[1]}\")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "logging.info(f\"Data preprocessing complete. Shape after preprocessing: {data_preprocessed.shape}\")\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "logging.info(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "logging.info(f\"Autoencoder initialized with input size {input_size}.\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Function to calculate reconstruction errors\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    logging.info(\"Reconstruction errors calculated.\")\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Step 2: Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Step 3: Dynamically calculate the threshold based on the mean and standard deviation of reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "\n",
    "# Set threshold as mean + 2 standard deviations\n",
    "threshold = mean_error + 2 * std_error\n",
    "logging.info(f\"Dynamically calculated threshold based on data: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies and get their corresponding row indices\n",
    "def detect_anomalies_with_indices(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # List to store indices of the anomalies\n",
    "    idx = 0  # Index counter for the data rows\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if loss.item() > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)  # Store the index of the anomaly\n",
    "        idx += 1  # Increment index counter\n",
    "    logging.info(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n",
    "    return anomalies, anomaly_indices\n",
    "\n",
    "# Detect anomalies and get their indices\n",
    "anomalies, anomaly_indices = detect_anomalies_with_indices(dataloader, model, threshold)\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Section with Logging\n",
    "# ==============================\n",
    "\n",
    "# A. Log Anomalies\n",
    "def log_anomalies(anomaly_indices, reconstruction_errors, data):\n",
    "    # Log anomalies with details\n",
    "    with open(\"anomaly_log.txt\", \"w\") as log_file:\n",
    "        for idx in anomaly_indices:\n",
    "            log_file.write(f\"Anomaly at row {idx}, Error: {reconstruction_errors[idx]}, Data: {data[idx]}\\n\")\n",
    "    logging.info(\"Anomalies logged.\")\n",
    "\n",
    "log_anomalies(anomaly_indices, reconstruction_errors, data_tensor)\n",
    "\n",
    "# B. Alert Driver if critical anomaly is detected\n",
    "def alert_driver(message):\n",
    "    logging.warning(f\"Driver Alert: {message}\")\n",
    "\n",
    "# If critical anomaly is detected (e.g., anomaly in sensor data, ECU data), alert driver\n",
    "for idx in anomaly_indices:\n",
    "    alert_driver(f\"Critical anomaly detected at row {idx}\")\n",
    "\n",
    "# C. Isolate the ECU if compromised (pseudo-code)\n",
    "def isolate_ecu(ecu_id):\n",
    "    logging.error(f\"ECU {ecu_id} is isolated due to suspicious activity.\")\n",
    "\n",
    "# Example: Isolate a specific ECU if anomaly is related to that ECU (replace 'ecu_id' with real data)\n",
    "isolate_ecu('ECU_1')\n",
    "\n",
    "# D. Trigger Safe Mode in case of critical failure\n",
    "def trigger_safe_mode():\n",
    "    logging.critical(\"Entering safe mode due to anomaly detection...\")\n",
    "\n",
    "# Trigger safe mode for severe anomalies\n",
    "if len(anomaly_indices) > 5:  # Example condition: more than 5 anomalies trigger safe mode\n",
    "    trigger_safe_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5137d4f-8e5b-4c0d-b4b5-930d50a69fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "\n",
    "# ==============================\n",
    "# Logging Configuration\n",
    "# ==============================\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Define the format\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"system_log.log\"),  # Save logs to a file\n",
    "        logging.StreamHandler()  # Also display logs on the console\n",
    "    ]\n",
    ")\n",
    "data_path = '../data/'\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=3000) \n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=3000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=3000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=3000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "logging.info(f\"Data loaded and concatenated. Total rows: {data.shape[0]}, Total columns: {data.shape[1]}\")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "logging.info(f\"Data preprocessing complete. Shape after preprocessing: {data_preprocessed.shape}\")\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "logging.info(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "logging.info(f\"Autoencoder initialized with input size {input_size}.\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Function to calculate reconstruction errors\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    logging.info(\"Reconstruction errors calculated.\")\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Step 2: Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Step 3: Dynamically calculate the threshold based on the mean and standard deviation of reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "\n",
    "# Set threshold as mean + 2 standard deviations\n",
    "threshold = mean_error + 2 * std_error\n",
    "logging.info(f\"Dynamically calculated threshold based on data: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies and get their corresponding row indices and reconstruction error values\n",
    "def detect_anomalies_with_values(data_loader, model, threshold):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # List to store indices of the anomalies\n",
    "    anomaly_values = []  # List to store reconstruction errors (anomaly values)\n",
    "    idx = 0  # Index counter for the data rows\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_error = loss.item()  # Reconstruction error for this batch\n",
    "        \n",
    "        # If the reconstruction error exceeds the threshold, flag as anomaly\n",
    "        if reconstruction_error > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)  # Store the index of the anomaly\n",
    "            anomaly_values.append(reconstruction_error)  # Store the value of the anomaly (reconstruction error)\n",
    "        idx += 1  # Increment index counter\n",
    "    \n",
    "    logging.info(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n",
    "    logging.info(f\"Anomaly values (reconstruction errors): {anomaly_values}\")\n",
    "    return anomalies, anomaly_indices, anomaly_values\n",
    "\n",
    "# Detect anomalies, their indices, and values\n",
    "anomalies, anomaly_indices, anomaly_values = detect_anomalies_with_values(dataloader, model, threshold)\n",
    "\n",
    "# Print the number of anomalies detected, their corresponding row indices, and their reconstruction error values\n",
    "print(f\"Number of anomalies detected: {len(anomalies)}\")\n",
    "print(f\"Anomalies detected at rows (indices): {anomaly_indices}\")\n",
    "print(f\"Anomaly values (reconstruction errors): {anomaly_values}\")\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Section with Logging\n",
    "# ==============================\n",
    "\n",
    "# A. Log Anomalies\n",
    "def log_anomalies(anomaly_indices, anomaly_values, data):\n",
    "    # Log anomalies with details\n",
    "    with open(\"anomaly_log.txt\", \"w\") as log_file:\n",
    "        for i, idx in enumerate(anomaly_indices):\n",
    "            log_file.write(f\"Anomaly at row {idx}, Error: {anomaly_values[i]}, Data: {data[idx]}\\n\")\n",
    "    logging.info(\"Anomalies logged.\")\n",
    "\n",
    "log_anomalies(anomaly_indices, anomaly_values, data_tensor)\n",
    "\n",
    "# B. Alert Driver if critical anomaly is detected\n",
    "def alert_driver(message):\n",
    "    logging.warning(f\"Driver Alert: {message}\")\n",
    "\n",
    "# If critical anomaly is detected (e.g., anomaly in sensor data, ECU data), alert driver\n",
    "for idx in anomaly_indices:\n",
    "    alert_driver(f\"Critical anomaly detected at row {idx}\")\n",
    "\n",
    "# C. Isolate the ECU if compromised (pseudo-code)\n",
    "def isolate_ecu(ecu_id):\n",
    "    logging.error(f\"ECU {ecu_id} is isolated due to suspicious activity.\")\n",
    "\n",
    "# Example: Isolate a specific ECU if anomaly is related to that ECU (replace 'ecu_id' with real data)\n",
    "isolate_ecu('ECU_1')\n",
    "\n",
    "# D. Trigger Safe Mode in case of critical failure\n",
    "def trigger_safe_mode():\n",
    "    logging.critical(\"Entering safe mode due to anomaly detection...\")\n",
    "\n",
    "# Trigger safe mode for severe anomalies\n",
    "if len(anomaly_indices) > 5:  # Example condition: more than 5 anomalies trigger safe mode\n",
    "    trigger_safe_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e813e3cf-e463-4e71-91fa-8104c86dc479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import time\n",
    "from fastapi import FastAPI\n",
    "\n",
    "# ==============================\n",
    "# Logging Configuration\n",
    "# ==============================\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"system_log.log\"),  # Save logs to a file\n",
    "        logging.StreamHandler()  # Also display logs on the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the datasets (replace 'path_to_*' with actual file paths)\n",
    "dos_data = pd.read_csv('path_to_DoS_dataset.csv')\n",
    "fuzzy_data = pd.read_csv('path_to_Fuzzy_dataset.csv')\n",
    "gear_data = pd.read_csv('path_to_gear_dataset.csv')\n",
    "rpm_data = pd.read_csv('path_to_RPM_dataset.csv')\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "logging.info(f\"Data loaded and concatenated. Total rows: {data.shape[0]}, Total columns: {data.shape[1]}\")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with the mean\n",
    "    ('scaler', StandardScaler())])  # Normalize the numeric features\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value (mode)\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])  # One-hot encode the categorical features\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "logging.info(f\"Data preprocessing complete. Shape after preprocessing: {data_preprocessed.shape}\")\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "logging.info(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "# ==============================\n",
    "# Autoencoder Model\n",
    "# ==============================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compressing input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        # Decoder: Reconstructing the original data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid()  # Using sigmoid to bring values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]  # Number of features (columns) after preprocessing\n",
    "model = Autoencoder(input_size)\n",
    "logging.info(f\"Autoencoder initialized with input size {input_size}.\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for reconstruction error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]  # DataLoader returns a tuple\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# ==============================\n",
    "# Anomaly Detection and Evaluation\n",
    "# ==============================\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())  # Collect the reconstruction error for each batch\n",
    "    logging.info(\"Reconstruction errors calculated.\")\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Calculate reconstruction errors for the training data\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# Calculate the threshold based on mean + 2 standard deviations of the reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "threshold = mean_error + 2 * std_error\n",
    "logging.info(f\"Dynamically calculated threshold: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies with values\n",
    "def detect_anomalies_with_values(data_loader, model, threshold):\n",
    "    model.eval()\n",
    "    anomalies = []\n",
    "    anomaly_indices = []  # Store indices of anomalies\n",
    "    anomaly_values = []  # Store reconstruction error values of anomalies\n",
    "    idx = 0\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_error = loss.item()\n",
    "        \n",
    "        # If the reconstruction error exceeds the threshold, flag it as an anomaly\n",
    "        if reconstruction_error > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)\n",
    "            anomaly_values.append(reconstruction_error)\n",
    "        idx += 1\n",
    "    \n",
    "    logging.info(f\"Anomalies detected at rows: {anomaly_indices}\")\n",
    "    logging.info(f\"Reconstruction error values of anomalies: {anomaly_values}\")\n",
    "    return anomalies, anomaly_indices, anomaly_values\n",
    "\n",
    "# Detect anomalies and their values\n",
    "anomalies, anomaly_indices, anomaly_values = detect_anomalies_with_values(dataloader, model, threshold)\n",
    "\n",
    "# ==============================\n",
    "# Real-Time Monitoring Simulation\n",
    "# ==============================\n",
    "def real_time_monitoring(data_loader, model, threshold):\n",
    "    model.eval()\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        reconstruction_error = criterion(outputs, inputs).item()\n",
    "        \n",
    "        # Log every batch in real-time\n",
    "        logging.info(f\"Real-time Reconstruction Error: {reconstruction_error}\")\n",
    "        \n",
    "        # If the error exceeds the threshold, trigger an alert\n",
    "        if reconstruction_error > threshold:\n",
    "            logging.warning(f\"Real-time Anomaly detected with error: {reconstruction_error}\")\n",
    "            alert_driver(f\"Real-time anomaly detected with error {reconstruction_error}\")\n",
    "            isolate_ecu('ECU_X')  # Simulate ECU isolation\n",
    "        time.sleep(1)  # Simulate real-time delay\n",
    "\n",
    "# Simulate real-time monitoring\n",
    "real_time_monitoring(dataloader, model, threshold)\n",
    "\n",
    "# ==============================\n",
    "# FastAPI REST API for Real-Time Anomaly Detection\n",
    "# ==============================\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: list):\n",
    "    input_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    output = model(input_tensor)\n",
    "    reconstruction_error = criterion(output, input_tensor).item()\n",
    "\n",
    "    # Check if it's an anomaly\n",
    "    is_anomaly = reconstruction_error > threshold\n",
    "\n",
    "    return {\"reconstruction_error\": reconstruction_error, \"is_anomaly\": is_anomaly}\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Functions\n",
    "# ==============================\n",
    "def alert_driver(message):\n",
    "    logging.warning(f\"Driver Alert: {message}\")\n",
    "\n",
    "def isolate_ecu(ecu_id):\n",
    "    logging.error(f\"ECU {ecu_id} isolated due to suspicious activity.\")\n",
    "\n",
    "def trigger_safe_mode():\n",
    "    logging.critical(\"Safe mode activated due to critical anomaly detection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fac51-62c4-4164-b244-4b42dbc046a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae592dd-c16d-456a-b752-d82bdbcb2dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef442d03-e8fc-406f-bc54-aa17a1035477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2bfb7-e780-4c1e-a5ef-f7396c58775e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d989f580-9b65-4363-8ca3-e12e06af5c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dfa45ed-3119-466d-9813-4c74386ccc05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T08:35:27.441339Z",
     "iopub.status.busy": "2024-09-23T08:35:27.441339Z",
     "iopub.status.idle": "2024-09-23T08:35:27.624281Z",
     "shell.execute_reply": "2024-09-23T08:35:27.624281Z",
     "shell.execute_reply.started": "2024-09-23T08:35:27.441339Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/DoS_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m dos_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDoS_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m) \n\u001b[0;32m     33\u001b[0m fuzzy_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuzzy_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     34\u001b[0m gear_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgear_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/DoS_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==============================\n",
    "# Logging Configuration\n",
    "# ==============================\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"system_log.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the datasets (replace 'path_to_*' with actual file paths)\n",
    "data_path = '../data/'\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=1000) \n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=1000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=1000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=1000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "logging.info(f\"Data loaded and concatenated. Total rows: {data.shape[0]}, Total columns: {data.shape[1]}\")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "logging.info(f\"Data preprocessing complete. Shape after preprocessing: {data_preprocessed.shape}\")\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "logging.info(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "# ==============================\n",
    "# Autoencoder Model\n",
    "# ==============================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]\n",
    "model = Autoencoder(input_size)\n",
    "logging.info(f\"Autoencoder initialized with input size {input_size}.\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# ==============================\n",
    "# Anomaly Detection and Evaluation\n",
    "# ==============================\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())\n",
    "    logging.info(\"Reconstruction errors calculated.\")\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# ==============================\n",
    "# Fine-Tuning Threshold with Percentile\n",
    "# ==============================\n",
    "def set_threshold_based_on_percentile(reconstruction_errors, percentile=95):\n",
    "    threshold = np.percentile(reconstruction_errors, percentile)\n",
    "    logging.info(f\"Threshold set at {percentile}th percentile: {threshold}\")\n",
    "    return threshold\n",
    "\n",
    "# Set the threshold based on the 95th percentile\n",
    "threshold = set_threshold_based_on_percentile(reconstruction_errors, percentile=95)\n",
    "\n",
    "# ==============================\n",
    "# Visualization of Reconstruction Errors\n",
    "# ==============================\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, color='blue', alpha=0.7)\n",
    "plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold: {threshold}')\n",
    "plt.title('Reconstruction Error Distribution with Percentile-Based Threshold')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# Anomaly Detection with Percentile-Based Threshold\n",
    "# ==============================\n",
    "def detect_anomalies_with_values(data_loader, model, threshold):\n",
    "    model.eval()\n",
    "    anomalies = []\n",
    "    anomaly_indices = []\n",
    "    anomaly_values = []\n",
    "    idx = 0\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_error = loss.item()\n",
    "        \n",
    "        if reconstruction_error > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)\n",
    "            anomaly_values.append(reconstruction_error)\n",
    "        idx += 1\n",
    "    \n",
    "    logging.info(f\"Anomalies detected at rows: {anomaly_indices}\")\n",
    "    logging.info(f\"Reconstruction error values of anomalies: {anomaly_values}\")\n",
    "    return anomalies, anomaly_indices, anomaly_values\n",
    "\n",
    "# Detect anomalies and their values based on the threshold\n",
    "anomalies, anomaly_indices, anomaly_values = detect_anomalies_with_values(dataloader, model, threshold)\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Process\n",
    "# ==============================\n",
    "def alert_driver(message):\n",
    "    logging.warning(f\"Driver Alert: {message}\")\n",
    "\n",
    "def isolate_ecu(ecu_id):\n",
    "    logging.error(f\"ECU {ecu_id} isolated due to suspicious activity.\")\n",
    "\n",
    "def trigger_safe_mode():\n",
    "    logging.critical(\"Safe mode activated due to critical anomaly detection.\")\n",
    "\n",
    "# A. Alert the driver for each detected anomaly\n",
    "for idx in anomaly_indices:\n",
    "    alert_driver(f\"Critical anomaly detected at row {idx}, reconstruction error: {anomaly_values[anomaly_indices.index(idx)]}\")\n",
    "\n",
    "# B. Isolate the ECU if anomalies are detected\n",
    "if len(anomaly_indices) > 0:\n",
    "    isolate_ecu('ECU_1')\n",
    "\n",
    "# C. Trigger safe mode if too many anomalies are detected\n",
    "if len(anomaly_indices) > 5:\n",
    "    trigger_safe_mode()\n",
    "\n",
    "# ==============================\n",
    "# Real-Time Monitoring Simulation\n",
    "# ==============================\n",
    "def real_time_monitoring(data_loader, model, threshold):\n",
    "    model.eval()\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        reconstruction_error = criterion(outputs, inputs).item()\n",
    "        \n",
    "        logging.info(f\"Real-time Reconstruction Error: {reconstruction_error}\")\n",
    "        \n",
    "        if reconstruction_error > threshold:\n",
    "            logging.warning(f\"Real-time Anomaly detected with error: {reconstruction_error}\")\n",
    "            alert_driver(f\"Real-time anomaly detected with error {reconstruction_error}\")\n",
    "            isolate_ecu('ECU_1')\n",
    "        time.sleep(1)\n",
    "\n",
    "# Simulate real-time monitoring\n",
    "real_time_monitoring(dataloader, model, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a6d82-377f-41f0-a5bc-f443d2c20de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c0647-521c-4875-8f2f-484f14aa2164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0dedbb-a46a-4a38-8488-9cc745dcdfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ==============================\n",
    "# Logging Configuration\n",
    "# ==============================\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"system_log.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# Load the datasets (replace 'path_to_*' with actual file paths)\n",
    "# ==============================\n",
    "data_path = './data/'\n",
    "\n",
    "# Load the datasets\n",
    "dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), nrows=3000) \n",
    "fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'),nrows=3000)\n",
    "gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), nrows=3000)\n",
    "rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), nrows=3000)\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], axis=0)\n",
    "logging.info(f\"Data loaded and concatenated. Total rows: {data.shape[0]}, Total columns: {data.shape[1]}\")\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing for numeric features (impute missing values with mean, scale them)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (impute missing values with mode, one-hot encode them)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "# Combine both transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Apply transformations to the data\n",
    "data_preprocessed = preprocessor.fit_transform(data)\n",
    "logging.info(f\"Data preprocessing complete. Shape after preprocessing: {data_preprocessed.shape}\")\n",
    "\n",
    "# Convert the processed data into a PyTorch tensor\n",
    "data_tensor = torch.tensor(data_preprocessed, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "logging.info(f\"DataLoader created with batch size {batch_size}.\")\n",
    "\n",
    "# ==============================\n",
    "# Autoencoder Model\n",
    "# ==============================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the Autoencoder\n",
    "input_size = data_preprocessed.shape[1]\n",
    "model = Autoencoder(input_size)\n",
    "logging.info(f\"Autoencoder initialized with input size {input_size}.\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the Autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    logging.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# ==============================\n",
    "# Anomaly Detection and Evaluation\n",
    "# ==============================\n",
    "def calculate_reconstruction_errors(data_loader, model):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_errors.append(loss.item())\n",
    "    logging.info(\"Reconstruction errors calculated.\")\n",
    "    return reconstruction_errors\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "reconstruction_errors = calculate_reconstruction_errors(dataloader, model)\n",
    "\n",
    "# ==============================\n",
    "# Fine-Tuning Threshold with Percentile\n",
    "# ==============================\n",
    "def set_threshold_based_on_percentile(reconstruction_errors, percentile=95):\n",
    "    threshold = np.percentile(reconstruction_errors, percentile)\n",
    "    logging.info(f\"Threshold set at {percentile}th percentile: {threshold}\")\n",
    "    return threshold\n",
    "\n",
    "# Set the threshold based on the 95th percentile\n",
    "threshold = set_threshold_based_on_percentile(reconstruction_errors, percentile=95)\n",
    "\n",
    "# ==============================\n",
    "# Visualization of Reconstruction Errors\n",
    "# ==============================\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(reconstruction_errors, bins=50, color='blue', alpha=0.7)\n",
    "plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold: {threshold}')\n",
    "plt.title('Reconstruction Error Distribution with Percentile-Based Threshold')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==============================\n",
    "# Anomaly Detection with Percentile-Based Threshold\n",
    "# ==============================\n",
    "def detect_anomalies_with_values(data_loader, model, threshold):\n",
    "    model.eval()\n",
    "    anomalies = []\n",
    "    anomaly_indices = []\n",
    "    anomaly_values = []\n",
    "    idx = 0\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        reconstruction_error = loss.item()\n",
    "        \n",
    "        if reconstruction_error > threshold:\n",
    "            anomalies.append(inputs)\n",
    "            anomaly_indices.append(idx)\n",
    "            anomaly_values.append(reconstruction_error)\n",
    "        idx += 1\n",
    "    \n",
    "    logging.info(f\"Anomalies detected at rows: {anomaly_indices}\")\n",
    "    logging.info(f\"Reconstruction error values of anomalies: {anomaly_values}\")\n",
    "    return anomalies, anomaly_indices, anomaly_values\n",
    "\n",
    "# Detect anomalies and their values based on the threshold\n",
    "anomalies, anomaly_indices, anomaly_values = detect_anomalies_with_values(dataloader, model, threshold)\n",
    "\n",
    "# ==============================\n",
    "# Mitigation Process\n",
    "# ==============================\n",
    "def alert_driver(message):\n",
    "    logging.warning(f\"Driver Alert: {message}\")\n",
    "\n",
    "def isolate_ecu(ecu_id):\n",
    "    logging.error(f\"ECU {ecu_id} isolated due to suspicious activity.\")\n",
    "\n",
    "def trigger_safe_mode():\n",
    "    logging.critical(\"Safe mode activated due to critical anomaly detection.\")\n",
    "\n",
    "# A. Alert the driver for each detected anomaly\n",
    "for idx in anomaly_indices:\n",
    "    alert_driver(f\"Critical anomaly detected at row {idx}, reconstruction error: {anomaly_values[anomaly_indices.index(idx)]}\")\n",
    "\n",
    "# B. Isolate the ECU if anomalies are detected\n",
    "if len(anomaly_indices) > 0:\n",
    "    isolate_ecu('ECU_1')\n",
    "\n",
    "# C. Trigger safe mode if too many anomalies are detected\n",
    "if len(anomaly_indices) > 5:\n",
    "    trigger_safe_mode()\n",
    "\n",
    "# ==============================\n",
    "# Real-Time Monitoring Simulation\n",
    "# ==============================\n",
    "def real_time_monitoring(data_loader, model, threshold):\n",
    "    model.eval()\n",
    "    for batch in data_loader:\n",
    "        inputs = batch[0]\n",
    "        outputs = model(inputs)\n",
    "        reconstruction_error = criterion(outputs, inputs).item()\n",
    "        \n",
    "        logging.info(f\"Real-time Reconstruction Error: {reconstruction_error}\")\n",
    "        \n",
    "        if reconstruction_error > threshold:\n",
    "            logging.warning(f\"Real-time Anomaly detected with error: {reconstruction_error}\")\n",
    "            alert_driver(f\"Real-time anomaly detected with error {reconstruction_error}\")\n",
    "            isolate_ecu('ECU_1')\n",
    "        time.sleep(1)\n",
    "\n",
    "# Simulate real-time monitoring\n",
    "real_time_monitoring(dataloader, model, threshold)\n",
    "\n",
    "# ==============================\n",
    "# FastAPI Integration for Real-Time Predictions\n",
    "# ==============================\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: list):\n",
    "    input_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    output = model(input_tensor)\n",
    "    reconstruction_error = criterion(output, input_tensor).item()\n",
    "\n",
    "    # Check if it's an anomaly\n",
    "    is_anomaly = reconstruction_error > threshold\n",
    "    return {\"reconstruction_error\": reconstruction_error, \"is_anomaly\": is_anomaly}\n",
    "\n",
    "# Run FastAPI with: uvicorn app:app --reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4969083-9557-4b35-b1cb-8c5ecc354745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffdc165-00a3-45ac-9bfb-f3026b930671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc0163-c3aa-4f3e-a1c6-dd73f724483b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b149628-0329-4b5e-a9eb-71c3773b2e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b55f4-08cc-4d92-8d6f-eab294105995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68ea6c6-212c-4aac-b139-ab9a64dcf367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07486b-9da8-488a-a6c9-48cedc33670b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518c1a5-f78c-4c1b-b15e-606970be8cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e4887-f7e6-452d-9eaa-6942baae848d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dca9dd-9684-4d97-ba40-b748431b8347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101dea6-96a9-4ac9-a234-06193d7a97e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
