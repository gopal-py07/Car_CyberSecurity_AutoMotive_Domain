{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33ec59f-dc5a-42b3-8dd6-3ad42d834d52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T15:40:07.047573Z",
     "iopub.status.busy": "2024-09-15T15:40:07.046577Z",
     "iopub.status.idle": "2024-09-15T15:43:23.512919Z",
     "shell.execute_reply": "2024-09-15T15:43:23.512919Z",
     "shell.execute_reply.started": "2024-09-15T15:40:07.047573Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 838. GiB for an array with shape (16569471, 6790) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(categorical_columns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     43\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Updated argument\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     categorical_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(combined_data[categorical_columns])\n\u001b[0;32m     45\u001b[0m     categorical_encoded_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(categorical_encoded, columns\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget_feature_names_out(categorical_columns))\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Concatenate the encoded categorical data with numeric data\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1063\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1057\u001b[0m out \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mcsr_matrix(\n\u001b[0;32m   1058\u001b[0m     (data, indices, indptr),\n\u001b[0;32m   1059\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(n_samples, feature_indices[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m   1060\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1061\u001b[0m )\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_output:\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1106\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1106\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1327\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 838. GiB for an array with shape (16569471, 6790) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting unsupervised learning data preprocessing.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'))\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'))\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'))\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'))\n",
    "    logging.info('Datasets loaded successfully.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')  # Updated argument\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "    \n",
    "    # Concatenate the encoded categorical data with numeric data\n",
    "    combined_data_encoded = pd.concat([combined_data[numeric_columns], categorical_encoded_df], axis=1)\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding.')\n",
    "else:\n",
    "    combined_data_encoded = combined_data[numeric_columns]\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the combined data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(combined_data_encoded)\n",
    "logging.info('Data normalized using StandardScaler.')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(X_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "logging.info(f'DataLoader created with batch size of 64. Total batches: {len(data_loader)}')\n",
    "\n",
    "# Print summary and log completion\n",
    "print(f\"Number of data points: {len(X_tensor)}\")\n",
    "print(f\"Number of batches in DataLoader: {len(data_loader)}\")\n",
    "logging.info('Data preprocessing completed successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2872fb-d0ea-44fc-86b9-96319b0275c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T15:47:41.204080Z",
     "iopub.status.busy": "2024-09-15T15:47:41.204080Z",
     "iopub.status.idle": "2024-09-15T15:48:34.857337Z",
     "shell.execute_reply": "2024-09-15T15:48:34.855735Z",
     "shell.execute_reply.started": "2024-09-15T15:47:41.204080Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.33 GiB for an array with shape (27, 16569471) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Separate numeric and categorical columns\u001b[39;00m\n\u001b[0;32m     36\u001b[0m numeric_columns \u001b[38;5;241m=\u001b[39m combined_data\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m---> 37\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m combined_data\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     39\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumeric columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategorical_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5079\u001b[0m, in \u001b[0;36mDataFrame.select_dtypes\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   5075\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   5077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 5079\u001b[0m mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39m_get_data_subset(predicate)\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   5080\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(mgr, axes\u001b[38;5;241m=\u001b[39mmgr\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    601\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 604\u001b[0m     res\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1786\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1786\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m _consolidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[0;32m   1787\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2267\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2265\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2267\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m _merge_blocks(\n\u001b[0;32m   2268\u001b[0m         \u001b[38;5;28mlist\u001b[39m(group_blocks), dtype\u001b[38;5;241m=\u001b[39mdtype, can_consolidate\u001b[38;5;241m=\u001b[39m_can_consolidate\n\u001b[0;32m   2269\u001b[0m     )\n\u001b[0;32m   2270\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2292\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2285\u001b[0m new_values: ArrayLike\n\u001b[0;32m   2287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   2288\u001b[0m     \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[0;32m   2289\u001b[0m     \u001b[38;5;66;03m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[0;32m   2290\u001b[0m     \u001b[38;5;66;03m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m     \u001b[38;5;66;03m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[1;32m-> 2292\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([b\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m blocks])  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2294\u001b[0m     bvals \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.33 GiB for an array with shape (27, 16569471) and data type object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "from scipy import sparse\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing1.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting unsupervised learning data preprocessing.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    dos_data = pd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'))\n",
    "    fuzzy_data = pd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'))\n",
    "    gear_data = pd.read_csv(os.path.join(data_path, 'gear_dataset.csv'))\n",
    "    rpm_data = pd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'))\n",
    "    logging.info('Datasets loaded successfully.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single DataFrame\n",
    "combined_data = pd.concat([dos_data, fuzzy_data, gear_data, rpm_data], ignore_index=True)\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data[numeric_columns])\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataset = TensorDataset(X_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "logging.info(f'DataLoader created with batch size of 64. Total batches: {len(data_loader)}')\n",
    "\n",
    "# Print summary and log completion\n",
    "print(f\"Number of data points: {len(X_tensor)}\")\n",
    "print(f\"Number of batches in DataLoader: {len(data_loader)}\")\n",
    "logging.info('Data preprocessing completed successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2821b040-fdc6-47d8-9302-f9e4dc0e889b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T15:52:50.685471Z",
     "iopub.status.busy": "2024-09-15T15:52:50.685471Z",
     "iopub.status.idle": "2024-09-15T15:52:55.527646Z",
     "shell.execute_reply": "2024-09-15T15:52:55.471473Z",
     "shell.execute_reply.started": "2024-09-15T15:52:50.685471Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------+--------+----------+\n| Column | Found  | Expected |\n+--------+--------+----------+\n| 00.1   | object | int64    |\n| 00.4   | object | float64  |\n+--------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- 00.1\n  ValueError(\"invalid literal for int() with base 10: '7f'\")\n- 00.4\n  ValueError(\"could not convert string to float: 'd1'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'00.1': 'object',\n       '00.4': 'object'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategorical_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Convert Dask DataFrame to a Pandas DataFrame for processing\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m combined_data_pandas \u001b[38;5;241m=\u001b[39m combined_data\u001b[38;5;241m.\u001b[39mcompute()  \u001b[38;5;66;03m# This will load the entire dataset into memory, but processed in chunks\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Handle categorical data using One-Hot Encoding with sparse matrix output\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(categorical_columns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask_expr\\_collection.py:476\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    475\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask_expr\\_expr.py:3723\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[1;34m(graph, name, *deps)\u001b[0m\n\u001b[0;32m   3721\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[0;32m   3722\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[1;32m-> 3723\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dask\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mget(graph, name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas_read_text(\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader,\n\u001b[0;32m    144\u001b[0m     block,\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[0;32m    146\u001b[0m     rest_kwargs,\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypes,\n\u001b[0;32m    148\u001b[0m     columns,\n\u001b[0;32m    149\u001b[0m     write_header,\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce,\n\u001b[0;32m    151\u001b[0m     path_info,\n\u001b[0;32m    152\u001b[0m )\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[1;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[1;32m--> 197\u001b[0m     coerce_dtypes(df, dtypes)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[1;34m(df, dtypes)\u001b[0m\n\u001b[0;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[0;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[0;32m    297\u001b[0m )\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------+--------+----------+\n| Column | Found  | Expected |\n+--------+--------+----------+\n| 00.1   | object | int64    |\n| 00.4   | object | float64  |\n+--------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- 00.1\n  ValueError(\"invalid literal for int() with base 10: '7f'\")\n- 00.4\n  ValueError(\"could not convert string to float: 'd1'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'00.1': 'object',\n       '00.4': 'object'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import sparse\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting unsupervised learning data preprocessing.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Load the datasets using Dask for efficient memory usage\n",
    "try:\n",
    "    dos_data = dd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'))\n",
    "    fuzzy_data = dd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'))\n",
    "    gear_data = dd.read_csv(os.path.join(data_path, 'gear_dataset.csv'))\n",
    "    rpm_data = dd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'))\n",
    "    logging.info('Datasets loaded successfully using Dask.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single Dask DataFrame\n",
    "combined_data = dd.concat([dos_data, fuzzy_data, gear_data, rpm_data])\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Check column types to identify numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Convert Dask DataFrame to a Pandas DataFrame for processing\n",
    "combined_data_pandas = combined_data.compute()  # This will load the entire dataset into memory, but processed in chunks\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data_pandas[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data_pandas[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data_pandas[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data_pandas[numeric_columns])\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataset = TensorDataset(X_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "logging.info(f'DataLoader created with batch size of 64. Total batches: {len(data_loader)}')\n",
    "\n",
    "# Print summary and log completion\n",
    "print(f\"Number of data points: {len(X_tensor)}\")\n",
    "print(f\"Number of batches in DataLoader: {len(data_loader)}\")\n",
    "logging.info('Data preprocessing completed successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "729b7896-95f7-4243-a065-a372947dcfc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-15T15:54:40.216649Z",
     "iopub.status.busy": "2024-09-15T15:54:40.215654Z",
     "iopub.status.idle": "2024-09-15T15:54:46.547686Z",
     "shell.execute_reply": "2024-09-15T15:54:46.547686Z",
     "shell.execute_reply.started": "2024-09-15T15:54:40.216649Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------+--------+----------+\n| Column | Found  | Expected |\n+--------+--------+----------+\n| 00.2   | object | int64    |\n+--------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- 00.2\n  ValueError(\"invalid literal for int() with base 10: 'd0'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'00.2': 'object'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategorical_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Convert Dask DataFrame to a Pandas DataFrame for processing\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m combined_data_pandas \u001b[38;5;241m=\u001b[39m combined_data\u001b[38;5;241m.\u001b[39mcompute()  \u001b[38;5;66;03m# This will load the entire dataset into memory, but processed in chunks\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Handle categorical data using One-Hot Encoding with sparse matrix output\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(categorical_columns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask_expr\\_collection.py:476\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    475\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask_expr\\_expr.py:3723\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[1;34m(graph, name, *deps)\u001b[0m\n\u001b[0;32m   3721\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[0;32m   3722\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[1;32m-> 3723\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dask\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mget(graph, name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas_read_text(\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader,\n\u001b[0;32m    144\u001b[0m     block,\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[0;32m    146\u001b[0m     rest_kwargs,\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypes,\n\u001b[0;32m    148\u001b[0m     columns,\n\u001b[0;32m    149\u001b[0m     write_header,\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce,\n\u001b[0;32m    151\u001b[0m     path_info,\n\u001b[0;32m    152\u001b[0m )\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[1;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[1;32m--> 197\u001b[0m     coerce_dtypes(df, dtypes)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[1;34m(df, dtypes)\u001b[0m\n\u001b[0;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[0;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[0;32m    297\u001b[0m )\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------+--------+----------+\n| Column | Found  | Expected |\n+--------+--------+----------+\n| 00.2   | object | int64    |\n+--------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- 00.2\n  ValueError(\"invalid literal for int() with base 10: 'd0'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'00.2': 'object'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy import sparse\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    filename='../logs/data_preprocessing.log',  \n",
    "    level=logging.INFO,                         \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  \n",
    ")\n",
    "\n",
    "logging.info('Starting unsupervised learning data preprocessing.')\n",
    "\n",
    "# Define the path to your data folder\n",
    "data_path = \"../data/\"\n",
    "\n",
    "# Specify the correct dtypes for problematic columns\n",
    "dtype_spec = {\n",
    "    '00.1': 'object',  # Treat this column as a string\n",
    "    '00.4': 'object'   # Treat this column as a string\n",
    "}\n",
    "\n",
    "# Load the datasets using Dask for efficient memory usage\n",
    "try:\n",
    "    dos_data = dd.read_csv(os.path.join(data_path, 'DoS_dataset.csv'), dtype=dtype_spec)\n",
    "    fuzzy_data = dd.read_csv(os.path.join(data_path, 'Fuzzy_dataset.csv'), dtype=dtype_spec)\n",
    "    gear_data = dd.read_csv(os.path.join(data_path, 'gear_dataset.csv'), dtype=dtype_spec)\n",
    "    rpm_data = dd.read_csv(os.path.join(data_path, 'RPM_dataset.csv'), dtype=dtype_spec)\n",
    "    logging.info('Datasets loaded successfully using Dask with specified dtypes.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading datasets: {e}\")\n",
    "\n",
    "# Combine datasets into a single Dask DataFrame\n",
    "combined_data = dd.concat([dos_data, fuzzy_data, gear_data, rpm_data])\n",
    "logging.info('Datasets combined successfully.')\n",
    "\n",
    "# Check column types to identify numeric and categorical columns\n",
    "numeric_columns = combined_data.select_dtypes(include=['number']).columns\n",
    "categorical_columns = combined_data.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "logging.info(f\"Numeric columns: {numeric_columns}\")\n",
    "logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Convert Dask DataFrame to a Pandas DataFrame for processing\n",
    "combined_data_pandas = combined_data.compute()  # This will load the entire dataset into memory, but processed in chunks\n",
    "\n",
    "# Handle categorical data using One-Hot Encoding with sparse matrix output\n",
    "if len(categorical_columns) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=True, drop='first')  # Keep output sparse to reduce memory usage\n",
    "    categorical_encoded = encoder.fit_transform(combined_data_pandas[categorical_columns])\n",
    "    \n",
    "    # Convert numeric data into a sparse format\n",
    "    numeric_data = combined_data_pandas[numeric_columns]\n",
    "    numeric_data_sparse = sparse.csr_matrix(numeric_data.values)\n",
    "    \n",
    "    # Concatenate sparse categorical and numeric data\n",
    "    combined_data_sparse = sparse.hstack([numeric_data_sparse, categorical_encoded])\n",
    "    logging.info('Categorical columns encoded using One-Hot Encoding (sparse format).')\n",
    "else:\n",
    "    combined_data_sparse = sparse.csr_matrix(combined_data_pandas[numeric_columns].values)\n",
    "    logging.info('No categorical columns to encode.')\n",
    "\n",
    "# Normalize the numeric data (can only apply normalization to the numeric columns)\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False is needed for sparse matrices\n",
    "numeric_data_scaled = scaler.fit_transform(combined_data_pandas[numeric_columns])\n",
    "\n",
    "# Convert scaled numeric data to sparse matrix and concatenate with the sparse categorical data\n",
    "numeric_data_scaled_sparse = sparse.csr_matrix(numeric_data_scaled)\n",
    "combined_data_encoded = sparse.hstack([numeric_data_scaled_sparse, categorical_encoded])\n",
    "\n",
    "# Convert the sparse matrix to a dense format for PyTorch tensor (for training)\n",
    "X_dense = combined_data_encoded.toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_dense, dtype=torch.float32)\n",
    "logging.info('Data converted to PyTorch tensors.')\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataset = TensorDataset(X_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "logging.info(f'DataLoader created with batch size of 64. Total batches: {len(data_loader)}')\n",
    "\n",
    "# Print summary and log completion\n",
    "print(f\"Number of data points: {len(X_tensor)}\")\n",
    "print(f\"Number of batches in DataLoader: {len(data_loader)}\")\n",
    "logging.info('Data preprocessing completed successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197e1f22-4900-4c00-956f-5e39d766be6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:36:22.654496Z",
     "iopub.status.busy": "2024-09-16T09:36:22.653493Z",
     "iopub.status.idle": "2024-09-16T09:36:29.142896Z",
     "shell.execute_reply": "2024-09-16T09:36:29.141888Z",
     "shell.execute_reply.started": "2024-09-16T09:36:22.654496Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/DoS_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Step 1: Load the DoS dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming DoS_dataset.csv contains data with numeric columns related to network activity\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/DoS_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 2: Preprocessing - Fill missing values and scale data\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Fill missing values (if any)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m data\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/DoS_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the DoS dataset\n",
    "# Assuming DoS_dataset.csv contains data with numeric columns related to network activity\n",
    "data = pd.read_csv('..data/DoS_dataset.csv')\n",
    "\n",
    "# Step 2: Preprocessing - Fill missing values and scale data\n",
    "# Fill missing values (if any)\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test = train_test_split(scaled_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Step 4: Define the Autoencoder model in PyTorch\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: Compress the input data\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(encoding_dim, 7),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # Decoder: Reconstruct the input data\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(7, encoding_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(encoding_dim, input_dim),\n",
    "            nn.Sigmoid()  # Use sigmoid for reconstruction between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Step 5: Initialize model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 14  # Compression factor (adjust as needed)\n",
    "model = Autoencoder(input_dim, encoding_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Step 6: Train the Autoencoder model\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        batch = X_train[i:i + batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Step 7: Anomaly detection - Calculate reconstruction error on the test set\n",
    "model.eval()  # Switch to evaluation mode\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(X_test)\n",
    "    mse = torch.mean((X_test - reconstructions) ** 2, dim=1)\n",
    "\n",
    "# Step 8: Set a threshold for anomaly detection\n",
    "threshold = torch.mean(mse) + 2 * torch.std(mse)\n",
    "\n",
    "# Step 9: Detect anomalies (samples with errors exceeding the threshold)\n",
    "anomalies = mse > threshold\n",
    "\n",
    "print(f\"Detected {torch.sum(anomalies).item()} anomalies out of {mse.shape[0]} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bf158-902f-4c64-b5f1-653c4da36024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
